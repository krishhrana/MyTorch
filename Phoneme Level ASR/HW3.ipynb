{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR4qfYrVoO4v"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA9qZoIDcx-h",
        "outputId": "a6e80561-dc35-45b4-fee6-bc20deed385d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONgAWhqdoYy-"
      },
      "source": [
        "### Levenshtein\n",
        "\n",
        "This may take a while"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS7a7xeEoaV9",
        "outputId": "6dc491f8-5c85-4252-b26a-2daddbcc3edf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.8/198.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.5/175.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCloning into 'ctcdecode'...\n",
            "remote: Enumerating objects: 1102, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 1102 (delta 16), reused 32 (delta 14), pack-reused 1063\u001b[K\n",
            "Receiving objects: 100% (1102/1102), 782.27 KiB | 25.23 MiB/s, done.\n",
            "Resolving deltas: 100% (529/529), done.\n",
            "Submodule 'third_party/ThreadPool' (https://github.com/progschj/ThreadPool.git) registered for path 'third_party/ThreadPool'\n",
            "Submodule 'third_party/kenlm' (https://github.com/kpu/kenlm.git) registered for path 'third_party/kenlm'\n",
            "Cloning into '/content/ctcdecode/third_party/ThreadPool'...\n",
            "remote: Enumerating objects: 82, done.        \n",
            "remote: Total 82 (delta 0), reused 0 (delta 0), pack-reused 82        \n",
            "Cloning into '/content/ctcdecode/third_party/kenlm'...\n",
            "remote: Enumerating objects: 14147, done.        \n",
            "remote: Counting objects: 100% (460/460), done.        \n",
            "remote: Compressing objects: 100% (320/320), done.        \n",
            "remote: Total 14147 (delta 152), reused 398 (delta 126), pack-reused 13687        \n",
            "Receiving objects: 100% (14147/14147), 5.91 MiB | 18.90 MiB/s, done.\n",
            "Resolving deltas: 100% (8032/8032), done.\n",
            "Submodule path 'third_party/ThreadPool': checked out '9a42ec1329f259a5f4881a291db1dcb8f2ad9040'\n",
            "Submodule path 'third_party/kenlm': checked out '35835f1ac4884126458ac89f9bf6dd9ccad561e0'\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "/content/ctcdecode\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb --quiet\n",
        "!pip install python-Levenshtein -q\n",
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!pip install wget -q\n",
        "%cd ctcdecode\n",
        "!pip install . -q\n",
        "%cd ..\n",
        "\n",
        "!pip install torchsummaryX -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWVONJxCobPc"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ZTCIXoof2f",
        "outputId": "1ada6319-1c89-4c66-cb9c-fd812a3c8031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torchaudio.transforms as tat\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# imports for decoding and distance calculation\n",
        "import ctcdecode\n",
        "import Levenshtein\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg3-yJ8tok34"
      },
      "source": [
        "# Kaggle Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdUelfGhom1m",
        "outputId": "fe5c6651-b67b-45c2-94a2-4101b1ed6d75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kaggle==1.5.8\n",
            "  Downloading kaggle-1.5.8.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.8-py3-none-any.whl size=73272 sha256=1b398fc8dc560c063076414ba168f48c92829180e674a34c4f33d78ad0caea51\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/02/ef/3f8c8d86b8d5388a1d3155876837f1a1a3143ab3fc2ff1ffad\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.13\n",
            "    Uninstalling kaggle-1.5.13:\n",
            "      Successfully uninstalled kaggle-1.5.13\n",
            "Successfully installed kaggle-1.5.8\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"krishrana1\",\"key\":\"ce91845d13bc142d6d456f7ef0f98c92\"}') # TODO: Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSjBwfXeoq4B",
        "outputId": "ecb97556-5de6-45ff-dd27-02b6808ebc74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 11-785-s23-hw3p2.zip to /content\n",
            "100% 15.9G/15.9G [13:43<00:00, 21.4MB/s]\n",
            "100% 15.9G/15.9G [13:43<00:00, 20.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c 11-785-s23-hw3p2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ruxWP60LCQA",
        "outputId": "81caee82-0622-47b6-aa52-8700239027da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11-785-s23-hw3p2  11-785-s23-hw3p2.zip\tctcdecode  sample_data\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This will take a couple minutes, but you should see at least the following:\n",
        "11-785-f22-hw3p2.zip  ctcdecode  hw3p2\n",
        "'''\n",
        "!unzip -q 11-785-s23-hw3p2.zip\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9v5ewZDMpYA"
      },
      "source": [
        "# Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Cp-716IMZRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a1344c-b05f-495f-f4a2-9264dcbc171b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ORNHnSFroP0"
      },
      "source": [
        "# Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0v7wHRWrqH6"
      },
      "outputs": [],
      "source": [
        "# ARPABET PHONEME MAPPING\n",
        "# DO NOT CHANGE\n",
        "# This overwrites the phonetics.py file.\n",
        "\n",
        "CMUdict_ARPAbet = {\n",
        "    \"\" : \" \",\n",
        "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\", \n",
        "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\", \n",
        "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\", \n",
        "    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\", \n",
        "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\", \n",
        "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\", \n",
        "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\", \n",
        "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
        "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"\n",
        "}\n",
        "\n",
        "CMUdict = list(CMUdict_ARPAbet.keys())\n",
        "ARPAbet = list(CMUdict_ARPAbet.values())\n",
        "\n",
        "\n",
        "PHONEMES = CMUdict[:-2]\n",
        "LABELS = ARPAbet[:-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eN2kcxwXLLBb"
      },
      "outputs": [],
      "source": [
        "# You might want to play around with the mapping as a sanity check here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(PHONEMES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEAGMHyuzvrm",
        "outputId": "3f95b8ba-ff89-49e1-9d82-bd0d0354fc16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "jY9x20w7yHzi",
        "outputId": "8ac97461-f2d8-4ab4-f059-3584241723ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-bd51ae7f1a22>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    Initializes the dataset.\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE7TmmHpa9yr"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    # For this homework, we give you full flexibility to design your data set class.\n",
        "    # Hint: The data from HW1 is very similar to this HW\n",
        "\n",
        "    #TODO\n",
        "    def __init__(self, root = '/content/11-785-s23-hw3p2/', partition = 'train-clean-100', context = 0, offset = 0): \n",
        "        '''\n",
        "        Initializes the dataset.\n",
        "\n",
        "        INPUTS: What inputs do you need here?\n",
        "        '''\n",
        "\n",
        "        self.mfcc_dir = root + partition + '/mfcc'\n",
        "        self.transcript_dir = root + partition + '/transcript'  \n",
        "\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "        self.transcript_files = sorted(os.listdir(self.transcript_dir))\n",
        "\n",
        "        self.PHONEMES = PHONEMES\n",
        "\n",
        "        '''\n",
        "        You may decide to do this in __getitem__ if you wish.\n",
        "        However, doing this here will make the __init__ function take the load of\n",
        "        loading the data, and shift it away from training.\n",
        "        '''\n",
        "\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "\n",
        "        for i in range(len(self.mfcc_files)):\n",
        "\n",
        "            mfcc        = np.load(self.mfcc_dir + \"/\" + self.mfcc_files[i])\n",
        "            mfcc = (mfcc - np.mean(mfcc, axis = 0)) / np.std(mfcc, axis = 0)\n",
        "    \n",
        "            transcript  = np.load(self.transcript_dir + \"/\" + self.transcript_files[i])\n",
        "\n",
        "            transcript = transcript[1:-1]\n",
        "\n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append(transcript)        \n",
        "\n",
        "        # NOTE:\n",
        "        # Each mfcc is of shape T1 x 27, T2 x 27, ...\n",
        "        # Each transcript is of shape (T1+2) x 27, (T2+2) x 27 before removing [SOS] and [EOS]\n",
        "\n",
        "        # TODO: Concatenate all mfccs in self.mfccs such that \n",
        "        # the final shape is T x 27 (Where T = T1 + T2 + ...) \n",
        "        # self.mfccs          = np.concatenate(self.mfccs, axis=0)\n",
        "\n",
        "        # TODO: Concatenate all transcripts in self.transcripts such that \n",
        "        # the final shape is (T,) meaning, each time step has one phoneme output\n",
        "        # self.transcripts    = np.concatenate(self.transcripts, axis=0)\n",
        "        # Hint: Use numpy to concatenate\n",
        "\n",
        "        # Length of the dataset is now the length of concatenated mfccs/transcripts\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "\n",
        "\n",
        "        t = []\n",
        "        for tr in self.transcripts:\n",
        "          t.append(np.array([self.PHONEMES.index(i) for i in tr]))\n",
        "        self.transcripts = t\n",
        "       \n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        TODO: What do we return here?\n",
        "        '''\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "\n",
        "        If you didn't do the loading and processing of the data in __init__,\n",
        "        do that here.\n",
        "\n",
        "        Once done, return a tuple of features and labels.\n",
        "        '''\n",
        "\n",
        "        mfcc = self.mfccs[ind] # TODO\n",
        "        transcript = self.transcripts[ind] # TODO\n",
        "        mfcc = torch.tensor(mfcc)\n",
        "        transcript = torch.tensor(transcript)\n",
        "        return mfcc, transcript\n",
        "\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "        '''\n",
        "        TODO:\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish. \n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lenghts of features, \n",
        "            and lengths of labels.\n",
        "        '''\n",
        "        # batch of input mfcc coefficients\n",
        "        batch_mfcc = [batch_item[0] for batch_item in batch] # TODO\n",
        "        # batch of output phonemes\n",
        "        batch_transcript = [batch_item[1] for batch_item in batch] # TODO\n",
        "\n",
        "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
        "        # Also be sure to check the input format (batch_first)\n",
        "        batch_mfcc_pad = torch.nn.utils.rnn.pad_sequence(batch_mfcc, padding_value=0.0, batch_first=True) # TODO\n",
        "        lengths_mfcc = [len(item) for item in batch_mfcc] # TODO \n",
        "        # lengths_mfcc = torch.tensor(lengths_mfcc)\n",
        "\n",
        "        batch_transcript_pad = torch.nn.utils.rnn.pad_sequence(batch_transcript, padding_value=0.0, batch_first=True) # TODO\n",
        "        lengths_transcript = [len(item) for item in batch_transcript] # TODO\n",
        "        # lengths_transcript = torch.tensor(lengths_transcript)\n",
        "\n",
        "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "        \n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n",
        "\n",
        "       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agmNBKf4JrLV"
      },
      "source": [
        "### Train Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqDrxeHfJw4g"
      },
      "source": [
        "### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrLS1wfVJppA"
      },
      "outputs": [],
      "source": [
        "# Test Dataloader\n",
        "class AudioDatasetTest(torch.utils.data.Dataset):\n",
        "    # For this homework, we give you full flexibility to design your data set class.\n",
        "    # Hint: The data from HW1 is very similar to this HW\n",
        "\n",
        "    #TODO\n",
        "    def __init__(self, root = '/content/11-785-s23-hw3p2/', partition = \"test-clean\"): \n",
        "        '''\n",
        "        Initializes the dataset.\n",
        "\n",
        "        INPUTS: What inputs do you need here?\n",
        "        '''\n",
        "\n",
        "        self.mfcc_dir = root + partition + '/mfcc'\n",
        "\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "\n",
        "        self.PHONEMES = PHONEMES\n",
        "\n",
        "        self.length = len(self.mfcc_files) \n",
        "\n",
        "\n",
        "        self.mfccs = []\n",
        "\n",
        "        for i in range(len(self.mfcc_files)):\n",
        "\n",
        "            mfcc = np.load(self.mfcc_dir + '/' + self.mfcc_files[i])\n",
        "            mfcc = (mfcc - np.mean(mfcc, axis = 0)) / np.std(mfcc, axis = 0)\n",
        "       \n",
        "            self.mfccs.append(mfcc)\n",
        "\n",
        "        # NOTE:\n",
        "        # Each mfcc is of shape T1 x 27, T2 x 27, ...\n",
        "        # Each transcript is of shape (T1+2) x 27, (T2+2) x 27 before removing [SOS] and [EOS]\n",
        "\n",
        "        # TODO: Concatenate all mfccs in self.mfccs such that \n",
        "        # the final shape is T x 27 (Where T = T1 + T2 + ...) \n",
        "        # self.mfccs          = np.concatenate(self.mfccs, axis=0)\n",
        "\n",
        "        # TODO: Concatenate all transcripts in self.transcripts such that \n",
        "        # the final shape is (T,) meaning, each time step has one phoneme output\n",
        "        # self.transcripts    = np.concatenate(self.transcripts, axis=0)\n",
        "        # Hint: Use numpy to concatenate\n",
        "\n",
        "        # Length of the dataset is now the length of concatenated mfccs/transcripts\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "       \n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        '''\n",
        "        TODO: What do we return here?\n",
        "        '''\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "\n",
        "        If you didn't do the loading and processing of the data in __init__,\n",
        "        do that here.\n",
        "\n",
        "        Once done, return a tuple of features and labels.\n",
        "        '''\n",
        "\n",
        "        mfcc = self.mfccs[ind] # TODO\n",
        "        mfcc = torch.FloatTensor(mfcc)\n",
        "        return mfcc\n",
        "\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "        '''\n",
        "        TODO:\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish. \n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lenghts of features, \n",
        "            and lengths of labels.\n",
        "        '''\n",
        "        # batch of input mfcc coefficients\n",
        "        batch_mfcc = batch\n",
        "\n",
        "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
        "        # Also be sure to check the input format (batch_first)\n",
        "        batch_mfcc_pad = torch.nn.utils.rnn.pad_sequence(batch_mfcc, padding_value=0.0, batch_first=True) # TODO\n",
        "        lengths_mfcc = [len(item) for item in batch_mfcc] # TODO \n",
        "\n",
        "\n",
        "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "        \n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return batch_mfcc_pad, torch.tensor(lengths_mfcc)\n",
        "\n",
        "       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt-veYcdL6Fe"
      },
      "source": [
        "### Data - Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4icymeX1ImUN"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64 # Increase if your device can handle it\n",
        "\n",
        "transforms = [] # set of tranformations\n",
        "# You may pass this as a parameter to the dataset class above\n",
        "# This will help modularize your implementation\n",
        "\n",
        "root = '/content/11-785-s23-hw3p2' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmuPk9J6L8dz"
      },
      "source": [
        "### Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_kG0gU2x4hH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac6b2482-2bbf-416d-eebb-8201043ded13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# get me RAMMM!!!! \n",
        "import gc \n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mzoYfTKu14s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b34b580b-1df9-402d-f78d-cde5e5490eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size:  64\n",
            "Train dataset samples = 28539, batches = 446\n",
            "Val dataset samples = 2703, batches = 43\n",
            "Test dataset samples = 2620, batches = 41\n"
          ]
        }
      ],
      "source": [
        "# Create objects for the dataset class\n",
        "train_data = AudioDataset(partition = 'train-clean-100') #TODO\n",
        "val_data = AudioDataset(partition='dev-clean') # TODO : You can either use the same class with some modifications or make a new one :)\n",
        "test_data = AudioDatasetTest() #TODO\n",
        "\n",
        "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
        "train_loader = DataLoader(train_data, shuffle = True, collate_fn = train_data.collate_fn, batch_size = BATCH_SIZE)\n",
        "val_loader = DataLoader(val_data, shuffle = True, collate_fn = val_data.collate_fn, batch_size = BATCH_SIZE)\n",
        "test_loader = DataLoader(test_data, collate_fn = test_data.collate_fn, batch_size = BATCH_SIZE)\n",
        "\n",
        "print(\"Batch size: \", BATCH_SIZE)\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUhY8XZcHyQI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce98d563-fb19-41cb-a348-a9a248d0c2c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test dataset samples = 2620, batches = 41\n"
          ]
        }
      ],
      "source": [
        "test_data = AudioDatasetTest() #TODO\n",
        "test_loader = DataLoader(test_data, collate_fn = test_data.collate_fn, batch_size = BATCH_SIZE)\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXMtwyviKaxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b220326-145c-4209-971d-aa8d30517a96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1661, 27]) torch.Size([64, 202]) torch.Size([64]) torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSexxhdfMUzx"
      },
      "source": [
        "# NETWORK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLad4pChcuvX"
      },
      "source": [
        "## Basic\n",
        "\n",
        "This is a basic block for understanding, you can skip this and move to pBLSTM one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ds7FZMz8Xcjs"
      },
      "outputs": [],
      "source": [
        "op = len(LABELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQhvHr71GJfq"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "class Network(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        # Adding some sort of embedding layer or feature extractor might help performance.\n",
        "        # self.embedding = ?\n",
        "        \n",
        "        # TODO : look up the documentation. You might need to pass some additional parameters.\n",
        "        self.lstm = nn.LSTM(input_size = 27, hidden_size = 256, num_layers = 3) \n",
        "       \n",
        "        self.classification = nn.Sequential(\n",
        "            torch.nn.Linear(256, op)\n",
        "        )\n",
        "\n",
        "        \n",
        "        self.logSoftmax = torch.nn.LogSoftmax(dim = 2)#TODO: Apply a log softmax here. Which dimension would apply it on ?\n",
        "\n",
        "    def forward(self, x, lx):\n",
        "        #TODO\n",
        "        # The forward function takes 2 parameter inputs here. Why?\n",
        "        # Refer to the handout for hints\n",
        "        packed_padding = pack_padded_sequence(x, lengths = lx, batch_first = True, enforce_sorted=False)\n",
        "        lstm_op = self.lstm(packed_padding)[0]\n",
        "        op, op_length = torch.nn.utils.rnn.pad_packed_sequence(lstm_op, batch_first = True)\n",
        "        op = self.classification(op)\n",
        "        op = self.logSoftmax(op)\n",
        "        return op, op_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB6eh3gnMUzy"
      },
      "source": [
        "## Pyramid Bi-LSTM (pBLSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd4BEX_yMUzz"
      },
      "outputs": [],
      "source": [
        "# Utils for network\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "class PermuteBlock(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.transpose(1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hEWFovVe6Ns"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "class LockedDropout(nn.Module):\n",
        "    def __init__(self, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.training or not self.dropout:\n",
        "            return x\n",
        "        x, x_lens = pad_packed_sequence(x, batch_first=True)\n",
        "        \n",
        "        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - self.dropout)\n",
        "        mask = Variable(m, requires_grad=False) / (1 - self.dropout)\n",
        "        mask = mask.expand_as(x)\n",
        "        return mask * x, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmdyXI6KMUzz"
      },
      "outputs": [],
      "source": [
        "\n",
        "class pBLSTM(torch.nn.Module):\n",
        "\n",
        "    '''\n",
        "    Pyramidal BiLSTM\n",
        "    Read the write up/paper and understand the concepts and then write your implementation here.\n",
        "\n",
        "    At each step,\n",
        "    1. Pad your input if it is packed (Unpack it)\n",
        "    2. Reduce the input length dimension by concatenating feature dimension\n",
        "        (Tip: Write down the shapes and understand)\n",
        "        (i) How should  you deal with odd/even length input? \n",
        "        (ii) How should you deal with input length array (x_lens) after truncating the input?\n",
        "    3. Pack your input\n",
        "    4. Pass it into LSTM layer\n",
        "    To make our implementation modular, we pass 1 layer at a time.\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(pBLSTM, self).__init__()\n",
        "\n",
        "        self.blstm = torch.nn.LSTM(batch_first = True, input_size = input_size * 2, hidden_size = hidden_size, \n",
        "                          num_layers = 3, bidirectional=True, dropout=0.2)\n",
        "\n",
        "    def forward(self, x_packed): # x_packed is a PackedSequence\n",
        "\n",
        "        # TODO: Pad Packed Sequence\n",
        "        x_pack_padded, x_lens = pad_packed_sequence(x_packed, batch_first=True)\n",
        "        #print(\"PBLSTM: \", x_pack_padded.shape)\n",
        "        \n",
        "        # Call self.trunc_reshape() which downsamples the time steps of x and increases the feature dimensions as mentioned above\n",
        "        x_downsample, x_lens = self.trunc_reshape(x_pack_padded, x_lens)\n",
        "\n",
        "        #print(\"Downsample: \", x_downsample.shape)\n",
        "        # self.trunc_reshape will return 2 outputs. What are they? Think about what quantites are changing.\n",
        "\n",
        "        # TODO: Pack Padded Sequence. What output(s) would you get?\n",
        "        x = pack_padded_sequence(x_downsample, x_lens, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        #print(\"PBLSTM: packed: \", x.data.shape)\n",
        "\n",
        "        # TODO: Pass the sequence through bLSTM\n",
        "        output, hidden = self.blstm(x)\n",
        "\n",
        "        # What do you return?\n",
        "        return output, hidden\n",
        "\n",
        "    def trunc_reshape(self, x, x_lens): \n",
        "        batch_size, max_len, feat_dim = x.size()\n",
        "        # TODO: If you have odd number of timesteps, how can you handle it? (Hint: You can exclude them)\n",
        "        #print(\"Max_len: \", max_len)\n",
        "        if max_len % 2 == 0:\n",
        "            ds = max_len // 2\n",
        "            x = x.view(batch_size, ds, feat_dim * 2)\n",
        "        else:\n",
        "            x = x[:, :-1, :]\n",
        "            ds = (max_len - 1) // 2\n",
        "            x = x.view(batch_size, (max_len - 1) // 2, feat_dim * 2)\n",
        "        # TODO: Reshape x. When reshaping x, you have to reduce number of timesteps by a downsampling factor while increasing number of features by the same factor\n",
        "        # TODO: Reduce lengths by the same downsampling factor\n",
        "        x_lens = torch.clamp(x_lens, max = ds)\n",
        "        return x, x_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3ZQ75OcMUz0"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEzw5_xmMUz0"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Encoder(torch.nn.Module):\n",
        "    '''\n",
        "    The Encoder takes utterances as inputs and returns latent feature representations\n",
        "    '''\n",
        "    def __init__(self, input_size, encoder_hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.embedding = torch.nn.Sequential(\n",
        "            # 27, 64\n",
        "            torch.nn.Conv1d(in_channels = input_size, out_channels = encoder_hidden_size, kernel_size = 3, stride = 1, padding = 1), \n",
        "            torch.nn.BatchNorm1d(encoder_hidden_size), \n",
        "            torch.nn.ReLU(), \n",
        "\n",
        "            torch.nn.Conv1d(in_channels = encoder_hidden_size, out_channels = encoder_hidden_size * 2, kernel_size = 3, stride = 1, padding = 1), \n",
        "            torch.nn.BatchNorm1d(encoder_hidden_size * 2), \n",
        "            torch.nn.ReLU(), \n",
        "\n",
        "            torch.nn.Conv1d(in_channels = encoder_hidden_size * 2, out_channels = encoder_hidden_size * 2, kernel_size = 3, stride = 1, padding = 1), \n",
        "            torch.nn.BatchNorm1d(encoder_hidden_size * 2), \n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "\n",
        "        #print(\"Size:\", encoder_hidden_size)\n",
        "        self.pblstm_0 = pBLSTM(input_size = encoder_hidden_size * 2, hidden_size = encoder_hidden_size * 2)\n",
        "\n",
        "        self.pBLSTMs = torch.nn.Sequential( \n",
        "            # TODO: Fill this up with pBLSTMs - What should the input_size be? \n",
        "\n",
        "            # Hint: You are downsampling timesteps by a factor of 2, upsampling features by a factor of 2 and the LSTM is bidirectional)\n",
        "            # Optional: Dropout/Locked Dropout after each pBLSTM (Not needed for early submission)\n",
        "            # https://github.com/salesforce/awd-lstm-lm/blob/dfd3cb0235d2caf2847a4d53e1cbd495b781b5d2/locked_dropout.py#L5\n",
        "            # ...\n",
        "            # ...\n",
        "            *[\n",
        "                pBLSTM(\n",
        "                    input_size = encoder_hidden_size * 2 * (2 ** i),\n",
        "                    hidden_size = encoder_hidden_size * 2 * (2 ** i)\n",
        "                )\n",
        "                for i in range(1, 2)\n",
        "            ]\n",
        "        )\n",
        "         \n",
        "    def forward(self, x, x_lens):\n",
        "        # Where are x and x_lens coming from? The dataloader\n",
        "        #TODO: Call the embedding layer\n",
        "\n",
        "        x = x.transpose(1, 2)\n",
        "        #print(\"27: \", x.shape)  # (B, 27, T)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        #print(\"64: \", x.shape)\n",
        "        x = x.transpose(1, 2)\n",
        "        \n",
        "        #print(\"64 transpose: \", x.shape) # (B, 64, T)\n",
        "\n",
        "        # Pack and pass to sequence model\n",
        "        # TODO: Pack Padded Sequence\n",
        "        x = pack_padded_sequence(x, x_lens, batch_first = True, enforce_sorted = False)\n",
        "\n",
        "        #print(x.data.shape)\n",
        "\n",
        "        # TODO: Pass Sequence through the pyramidal Bi-LSTM layer\n",
        "        x, _ = self.pblstm_0.forward(x)\n",
        "        for layer in self.pBLSTMs:\n",
        "            x, _ = layer.forward(x)\n",
        "        # TODO: Pad Packed Sequence\n",
        "        x, encoder_lens = pad_packed_sequence(x, batch_first = True)\n",
        "        encoder_outputs = x\n",
        "\n",
        "        # Remember the number of output(s) each function returns\n",
        "\n",
        "        return encoder_outputs, encoder_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg82HXa3MUz1"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQIRxdNTMUz1"
      },
      "outputs": [],
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, output_size= 41):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            PermuteBlock(), \n",
        "            torch.nn.BatchNorm1d(embed_size), \n",
        "            PermuteBlock(),\n",
        "            #TODO define your MLP arch. Refer HW1P2\n",
        "            torch.nn.Linear(embed_size, 512), \n",
        "            torch.nn.ReLU(), \n",
        "            torch.nn.Dropout(0.2),\n",
        "            PermuteBlock(), \n",
        "            torch.nn.BatchNorm1d(512), \n",
        "            PermuteBlock(),\n",
        "            torch.nn.Linear(512, 128), \n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(p=0.2),\n",
        "            PermuteBlock(), \n",
        "            torch.nn.BatchNorm1d(128), \n",
        "            PermuteBlock(),\n",
        "            torch.nn.Linear(128, output_size)\n",
        "            #Use Permute Block before and after BatchNorm1d() to match the size\n",
        "        )\n",
        "        \n",
        "        self.softmax = torch.nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, encoder_out):\n",
        "        #TODO call your MLP\n",
        "        output = self.mlp(encoder_out)\n",
        "        output = self.softmax(output)\n",
        "        #TODO Think what should be the final output of the decoder for the classification \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmHf6pFiMUz1"
      },
      "outputs": [],
      "source": [
        "class ASRModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, embed_size = 512, output_size= len(PHONEMES)):\n",
        "        super().__init__()\n",
        "\n",
        "        self.augmentations  = torch.nn.Sequential(\n",
        "            tat.TimeMasking(20),\n",
        "            tat.FrequencyMasking(20)\n",
        "            #TODO Add Time Masking/ Frequency Masking\n",
        "            #Hint: See how to use PermuteBlock() function defined above\n",
        "        )\n",
        "        self.encoder        = Encoder(input_size, 64)# TODO: Initialize Encoder\n",
        "        self.decoder        = Decoder(embed_size, output_size)# TODO: Initialize Decoder \n",
        "\n",
        "        \n",
        "    \n",
        "    def forward(self, x, lengths_x):\n",
        "        \n",
        "        if self.training:\n",
        "            x = self.augmentations(x)\n",
        "\n",
        "        #print(\"ASR: \", x.shape)\n",
        "        #print(\"ASR: \", lengths_x.shape)\n",
        "\n",
        "\n",
        "        encoder_out, encoder_lens   = self.encoder.forward(x, lengths_x)\n",
        "        decoder_out                 = self.decoder.forward(encoder_out)\n",
        "\n",
        "        return decoder_out, encoder_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUThsowyQdN7"
      },
      "source": [
        "## INIT\n",
        "(If trying out the basic Network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "CGoiXd70tb5z",
        "outputId": "073d1744-c2a0-4133-b63d-d5a91632b7ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================\n",
            "                          Kernel Shape    Output Shape     Params  Mult-Adds\n",
            "Layer                                                                       \n",
            "0_lstm                               -    [79746, 256]  1.344512M  1.338368M\n",
            "1_classification.Linear_0    [256, 41]  [64, 1661, 41]    10.537k    10.496k\n",
            "2_logSoftmax                         -  [64, 1661, 41]          -          -\n",
            "----------------------------------------------------------------------------\n",
            "                         Totals\n",
            "Total params          1.355049M\n",
            "Trainable params      1.355049M\n",
            "Non-trainable params        0.0\n",
            "Mult-Adds             1.348864M\n",
            "============================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          Kernel Shape    Output Shape     Params  Mult-Adds\n",
              "Layer                                                                       \n",
              "0_lstm                               -    [79746, 256]  1344512.0  1338368.0\n",
              "1_classification.Linear_0    [256, 41]  [64, 1661, 41]    10537.0    10496.0\n",
              "2_logSoftmax                         -  [64, 1661, 41]        NaN        NaN"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6ce524e9-5367-4d30-921b-823dacf73c84\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_lstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[79746, 256]</td>\n",
              "      <td>1344512.0</td>\n",
              "      <td>1338368.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_classification.Linear_0</th>\n",
              "      <td>[256, 41]</td>\n",
              "      <td>[64, 1661, 41]</td>\n",
              "      <td>10537.0</td>\n",
              "      <td>10496.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_logSoftmax</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 1661, 41]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6ce524e9-5367-4d30-921b-823dacf73c84')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6ce524e9-5367-4d30-921b-823dacf73c84 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6ce524e9-5367-4d30-921b-823dacf73c84');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "model = Network().to(device)\n",
        "summary(model, x.to(device), lx) # x and lx come from the sanity check above :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV7DMPDoMUz2"
      },
      "source": [
        "## INIT ASR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oaaDsnnLMUz2",
        "outputId": "3647e7c7-15c6-44b3-db1d-cf515cb1ae03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ASRModel(\n",
            "  (augmentations): Sequential(\n",
            "    (0): TimeMasking()\n",
            "    (1): FrequencyMasking()\n",
            "  )\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Sequential(\n",
            "      (0): Conv1d(27, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU()\n",
            "      (6): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "      (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (8): ReLU()\n",
            "    )\n",
            "    (pblstm_0): pBLSTM(\n",
            "      (blstm): LSTM(256, 128, num_layers=3, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "    )\n",
            "    (pBLSTMs): Sequential(\n",
            "      (0): pBLSTM(\n",
            "        (blstm): LSTM(512, 256, num_layers=3, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (mlp): Sequential(\n",
            "      (0): PermuteBlock()\n",
            "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): PermuteBlock()\n",
            "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (4): ReLU()\n",
            "      (5): Dropout(p=0.2, inplace=False)\n",
            "      (6): PermuteBlock()\n",
            "      (7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (8): PermuteBlock()\n",
            "      (9): Linear(in_features=512, out_features=128, bias=True)\n",
            "      (10): ReLU()\n",
            "      (11): Dropout(p=0.2, inplace=False)\n",
            "      (12): PermuteBlock()\n",
            "      (13): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (14): PermuteBlock()\n",
            "      (15): Linear(in_features=128, out_features=41, bias=True)\n",
            "    )\n",
            "    (softmax): LogSoftmax(dim=2)\n",
            "  )\n",
            ")\n",
            "=========================================================================================\n",
            "                                     Kernel Shape     Output Shape     Params  \\\n",
            "Layer                                                                           \n",
            "0_augmentations.TimeMasking_0                   -   [64, 1661, 27]          -   \n",
            "1_augmentations.FrequencyMasking_1              -   [64, 1661, 27]          -   \n",
            "2_encoder.embedding.Conv1d_0          [27, 64, 3]   [64, 64, 1661]     5.248k   \n",
            "3_encoder.embedding.BatchNorm1d_1            [64]   [64, 64, 1661]      128.0   \n",
            "4_encoder.embedding.ReLU_2                      -   [64, 64, 1661]          -   \n",
            "5_encoder.embedding.Conv1d_3         [64, 128, 3]  [64, 128, 1661]    24.704k   \n",
            "6_encoder.embedding.BatchNorm1d_4           [128]  [64, 128, 1661]      256.0   \n",
            "7_encoder.embedding.ReLU_5                      -  [64, 128, 1661]          -   \n",
            "8_encoder.embedding.Conv1d_6        [128, 128, 3]  [64, 128, 1661]     49.28k   \n",
            "9_encoder.embedding.BatchNorm1d_7           [128]  [64, 128, 1661]      256.0   \n",
            "10_encoder.embedding.ReLU_8                     -  [64, 128, 1661]          -   \n",
            "11_encoder.pblstm_0.LSTM_blstm                  -     [49484, 256]  1.185792M   \n",
            "12_encoder.pBLSTMs.0.LSTM_blstm                 -     [26093, 512]   4.73088M   \n",
            "13_decoder.mlp.PermuteBlock_0                   -   [64, 512, 415]          -   \n",
            "14_decoder.mlp.BatchNorm1d_1                [512]   [64, 512, 415]     1.024k   \n",
            "15_decoder.mlp.PermuteBlock_2                   -   [64, 415, 512]          -   \n",
            "16_decoder.mlp.Linear_3                [512, 512]   [64, 415, 512]   262.656k   \n",
            "17_decoder.mlp.ReLU_4                           -   [64, 415, 512]          -   \n",
            "18_decoder.mlp.Dropout_5                        -   [64, 415, 512]          -   \n",
            "19_decoder.mlp.PermuteBlock_6                   -   [64, 512, 415]          -   \n",
            "20_decoder.mlp.BatchNorm1d_7                [512]   [64, 512, 415]     1.024k   \n",
            "21_decoder.mlp.PermuteBlock_8                   -   [64, 415, 512]          -   \n",
            "22_decoder.mlp.Linear_9                [512, 128]   [64, 415, 128]    65.664k   \n",
            "23_decoder.mlp.ReLU_10                          -   [64, 415, 128]          -   \n",
            "24_decoder.mlp.Dropout_11                       -   [64, 415, 128]          -   \n",
            "25_decoder.mlp.PermuteBlock_12                  -   [64, 128, 415]          -   \n",
            "26_decoder.mlp.BatchNorm1d_13               [128]   [64, 128, 415]      256.0   \n",
            "27_decoder.mlp.PermuteBlock_14                  -   [64, 415, 128]          -   \n",
            "28_decoder.mlp.Linear_15                [128, 41]    [64, 415, 41]     5.289k   \n",
            "29_decoder.LogSoftmax_softmax                   -    [64, 415, 41]          -   \n",
            "\n",
            "                                     Mult-Adds  \n",
            "Layer                                           \n",
            "0_augmentations.TimeMasking_0                -  \n",
            "1_augmentations.FrequencyMasking_1           -  \n",
            "2_encoder.embedding.Conv1d_0         8.610624M  \n",
            "3_encoder.embedding.BatchNorm1d_1         64.0  \n",
            "4_encoder.embedding.ReLU_2                   -  \n",
            "5_encoder.embedding.Conv1d_3        40.820736M  \n",
            "6_encoder.embedding.BatchNorm1d_4        128.0  \n",
            "7_encoder.embedding.ReLU_5                   -  \n",
            "8_encoder.embedding.Conv1d_6        81.641472M  \n",
            "9_encoder.embedding.BatchNorm1d_7        128.0  \n",
            "10_encoder.embedding.ReLU_8                  -  \n",
            "11_encoder.pblstm_0.LSTM_blstm       1.179648M  \n",
            "12_encoder.pBLSTMs.0.LSTM_blstm      4.718592M  \n",
            "13_decoder.mlp.PermuteBlock_0                -  \n",
            "14_decoder.mlp.BatchNorm1d_1             512.0  \n",
            "15_decoder.mlp.PermuteBlock_2                -  \n",
            "16_decoder.mlp.Linear_3               262.144k  \n",
            "17_decoder.mlp.ReLU_4                        -  \n",
            "18_decoder.mlp.Dropout_5                     -  \n",
            "19_decoder.mlp.PermuteBlock_6                -  \n",
            "20_decoder.mlp.BatchNorm1d_7             512.0  \n",
            "21_decoder.mlp.PermuteBlock_8                -  \n",
            "22_decoder.mlp.Linear_9                65.536k  \n",
            "23_decoder.mlp.ReLU_10                       -  \n",
            "24_decoder.mlp.Dropout_11                    -  \n",
            "25_decoder.mlp.PermuteBlock_12               -  \n",
            "26_decoder.mlp.BatchNorm1d_13            128.0  \n",
            "27_decoder.mlp.PermuteBlock_14               -  \n",
            "28_decoder.mlp.Linear_15                5.248k  \n",
            "29_decoder.LogSoftmax_softmax                -  \n",
            "-----------------------------------------------------------------------------------------\n",
            "                           Totals\n",
            "Total params            6.332457M\n",
            "Trainable params        6.332457M\n",
            "Non-trainable params          0.0\n",
            "Mult-Adds             137.305472M\n",
            "=========================================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     Kernel Shape     Output Shape     Params  \\\n",
              "Layer                                                                           \n",
              "0_augmentations.TimeMasking_0                   -   [64, 1661, 27]        NaN   \n",
              "1_augmentations.FrequencyMasking_1              -   [64, 1661, 27]        NaN   \n",
              "2_encoder.embedding.Conv1d_0          [27, 64, 3]   [64, 64, 1661]     5248.0   \n",
              "3_encoder.embedding.BatchNorm1d_1            [64]   [64, 64, 1661]      128.0   \n",
              "4_encoder.embedding.ReLU_2                      -   [64, 64, 1661]        NaN   \n",
              "5_encoder.embedding.Conv1d_3         [64, 128, 3]  [64, 128, 1661]    24704.0   \n",
              "6_encoder.embedding.BatchNorm1d_4           [128]  [64, 128, 1661]      256.0   \n",
              "7_encoder.embedding.ReLU_5                      -  [64, 128, 1661]        NaN   \n",
              "8_encoder.embedding.Conv1d_6        [128, 128, 3]  [64, 128, 1661]    49280.0   \n",
              "9_encoder.embedding.BatchNorm1d_7           [128]  [64, 128, 1661]      256.0   \n",
              "10_encoder.embedding.ReLU_8                     -  [64, 128, 1661]        NaN   \n",
              "11_encoder.pblstm_0.LSTM_blstm                  -     [49484, 256]  1185792.0   \n",
              "12_encoder.pBLSTMs.0.LSTM_blstm                 -     [26093, 512]  4730880.0   \n",
              "13_decoder.mlp.PermuteBlock_0                   -   [64, 512, 415]        NaN   \n",
              "14_decoder.mlp.BatchNorm1d_1                [512]   [64, 512, 415]     1024.0   \n",
              "15_decoder.mlp.PermuteBlock_2                   -   [64, 415, 512]        NaN   \n",
              "16_decoder.mlp.Linear_3                [512, 512]   [64, 415, 512]   262656.0   \n",
              "17_decoder.mlp.ReLU_4                           -   [64, 415, 512]        NaN   \n",
              "18_decoder.mlp.Dropout_5                        -   [64, 415, 512]        NaN   \n",
              "19_decoder.mlp.PermuteBlock_6                   -   [64, 512, 415]        NaN   \n",
              "20_decoder.mlp.BatchNorm1d_7                [512]   [64, 512, 415]     1024.0   \n",
              "21_decoder.mlp.PermuteBlock_8                   -   [64, 415, 512]        NaN   \n",
              "22_decoder.mlp.Linear_9                [512, 128]   [64, 415, 128]    65664.0   \n",
              "23_decoder.mlp.ReLU_10                          -   [64, 415, 128]        NaN   \n",
              "24_decoder.mlp.Dropout_11                       -   [64, 415, 128]        NaN   \n",
              "25_decoder.mlp.PermuteBlock_12                  -   [64, 128, 415]        NaN   \n",
              "26_decoder.mlp.BatchNorm1d_13               [128]   [64, 128, 415]      256.0   \n",
              "27_decoder.mlp.PermuteBlock_14                  -   [64, 415, 128]        NaN   \n",
              "28_decoder.mlp.Linear_15                [128, 41]    [64, 415, 41]     5289.0   \n",
              "29_decoder.LogSoftmax_softmax                   -    [64, 415, 41]        NaN   \n",
              "\n",
              "                                     Mult-Adds  \n",
              "Layer                                           \n",
              "0_augmentations.TimeMasking_0              NaN  \n",
              "1_augmentations.FrequencyMasking_1         NaN  \n",
              "2_encoder.embedding.Conv1d_0         8610624.0  \n",
              "3_encoder.embedding.BatchNorm1d_1         64.0  \n",
              "4_encoder.embedding.ReLU_2                 NaN  \n",
              "5_encoder.embedding.Conv1d_3        40820736.0  \n",
              "6_encoder.embedding.BatchNorm1d_4        128.0  \n",
              "7_encoder.embedding.ReLU_5                 NaN  \n",
              "8_encoder.embedding.Conv1d_6        81641472.0  \n",
              "9_encoder.embedding.BatchNorm1d_7        128.0  \n",
              "10_encoder.embedding.ReLU_8                NaN  \n",
              "11_encoder.pblstm_0.LSTM_blstm       1179648.0  \n",
              "12_encoder.pBLSTMs.0.LSTM_blstm      4718592.0  \n",
              "13_decoder.mlp.PermuteBlock_0              NaN  \n",
              "14_decoder.mlp.BatchNorm1d_1             512.0  \n",
              "15_decoder.mlp.PermuteBlock_2              NaN  \n",
              "16_decoder.mlp.Linear_3               262144.0  \n",
              "17_decoder.mlp.ReLU_4                      NaN  \n",
              "18_decoder.mlp.Dropout_5                   NaN  \n",
              "19_decoder.mlp.PermuteBlock_6              NaN  \n",
              "20_decoder.mlp.BatchNorm1d_7             512.0  \n",
              "21_decoder.mlp.PermuteBlock_8              NaN  \n",
              "22_decoder.mlp.Linear_9                65536.0  \n",
              "23_decoder.mlp.ReLU_10                     NaN  \n",
              "24_decoder.mlp.Dropout_11                  NaN  \n",
              "25_decoder.mlp.PermuteBlock_12             NaN  \n",
              "26_decoder.mlp.BatchNorm1d_13            128.0  \n",
              "27_decoder.mlp.PermuteBlock_14             NaN  \n",
              "28_decoder.mlp.Linear_15                5248.0  \n",
              "29_decoder.LogSoftmax_softmax              NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d90d3bb7-ed6f-4faa-b3d5-da7c6135af05\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_augmentations.TimeMasking_0</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 1661, 27]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_augmentations.FrequencyMasking_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 1661, 27]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_encoder.embedding.Conv1d_0</th>\n",
              "      <td>[27, 64, 3]</td>\n",
              "      <td>[64, 64, 1661]</td>\n",
              "      <td>5248.0</td>\n",
              "      <td>8610624.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_encoder.embedding.BatchNorm1d_1</th>\n",
              "      <td>[64]</td>\n",
              "      <td>[64, 64, 1661]</td>\n",
              "      <td>128.0</td>\n",
              "      <td>64.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_encoder.embedding.ReLU_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 64, 1661]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_encoder.embedding.Conv1d_3</th>\n",
              "      <td>[64, 128, 3]</td>\n",
              "      <td>[64, 128, 1661]</td>\n",
              "      <td>24704.0</td>\n",
              "      <td>40820736.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_encoder.embedding.BatchNorm1d_4</th>\n",
              "      <td>[128]</td>\n",
              "      <td>[64, 128, 1661]</td>\n",
              "      <td>256.0</td>\n",
              "      <td>128.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_encoder.embedding.ReLU_5</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 128, 1661]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8_encoder.embedding.Conv1d_6</th>\n",
              "      <td>[128, 128, 3]</td>\n",
              "      <td>[64, 128, 1661]</td>\n",
              "      <td>49280.0</td>\n",
              "      <td>81641472.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_encoder.embedding.BatchNorm1d_7</th>\n",
              "      <td>[128]</td>\n",
              "      <td>[64, 128, 1661]</td>\n",
              "      <td>256.0</td>\n",
              "      <td>128.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_encoder.embedding.ReLU_8</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 128, 1661]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11_encoder.pblstm_0.LSTM_blstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[49484, 256]</td>\n",
              "      <td>1185792.0</td>\n",
              "      <td>1179648.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12_encoder.pBLSTMs.0.LSTM_blstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[26093, 512]</td>\n",
              "      <td>4730880.0</td>\n",
              "      <td>4718592.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13_decoder.mlp.PermuteBlock_0</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 512, 415]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14_decoder.mlp.BatchNorm1d_1</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[64, 512, 415]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15_decoder.mlp.PermuteBlock_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 415, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16_decoder.mlp.Linear_3</th>\n",
              "      <td>[512, 512]</td>\n",
              "      <td>[64, 415, 512]</td>\n",
              "      <td>262656.0</td>\n",
              "      <td>262144.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17_decoder.mlp.ReLU_4</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 415, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18_decoder.mlp.Dropout_5</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 415, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19_decoder.mlp.PermuteBlock_6</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 512, 415]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_decoder.mlp.BatchNorm1d_7</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[64, 512, 415]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21_decoder.mlp.PermuteBlock_8</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 415, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22_decoder.mlp.Linear_9</th>\n",
              "      <td>[512, 128]</td>\n",
              "      <td>[64, 415, 128]</td>\n",
              "      <td>65664.0</td>\n",
              "      <td>65536.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23_decoder.mlp.ReLU_10</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 415, 128]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24_decoder.mlp.Dropout_11</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 415, 128]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25_decoder.mlp.PermuteBlock_12</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 128, 415]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26_decoder.mlp.BatchNorm1d_13</th>\n",
              "      <td>[128]</td>\n",
              "      <td>[64, 128, 415]</td>\n",
              "      <td>256.0</td>\n",
              "      <td>128.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27_decoder.mlp.PermuteBlock_14</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 415, 128]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28_decoder.mlp.Linear_15</th>\n",
              "      <td>[128, 41]</td>\n",
              "      <td>[64, 415, 41]</td>\n",
              "      <td>5289.0</td>\n",
              "      <td>5248.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29_decoder.LogSoftmax_softmax</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 415, 41]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d90d3bb7-ed6f-4faa-b3d5-da7c6135af05')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d90d3bb7-ed6f-4faa-b3d5-da7c6135af05 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d90d3bb7-ed6f-4faa-b3d5-da7c6135af05');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "model = ASRModel(input_size  = 27, embed_size  = 512, output_size = len(PHONEMES)).to(device)\n",
        "print(model)\n",
        "summary(model, x.to(device), lx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ6oKHyTLjqN"
      },
      "outputs": [],
      "source": [
        "ckpt = torch.load('/content/best_model-5.pth')\n",
        "model.load_state_dict(ckpt['model_state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN82c3KpLup8"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"beam_width\" : 2,\n",
        "    \"lr\" : 2e-3,\n",
        "    \"epochs\" : 100\n",
        "    } # Feel free to add more items here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGoozH2nd6KB"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "\n",
        "\n",
        "criterion = torch.nn.CTCLoss(zero_infinity = True)\n",
        "# Define CTC loss as the criterion. How would the losses be reduced?\n",
        "# CTC Loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
        "# Refer to the handout for hints\n",
        "\n",
        "optimizer =  torch.optim.AdamW(model.parameters(), config['lr']) # What goes in here?\n",
        "#optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "\n",
        "# Declare the decoder. Use the CTC Beam Decoder to decode phonemes\n",
        "# CTC Beam Decoder Doc: https://github.com/parlance/ctcdecode\n",
        "decoder = CTCBeamDecoder(LABELS, beam_width=config['beam_width'], log_probs_input=True)#TODO \n",
        "\n",
        "scheduler =  torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.7, patience=2) #TODO\n",
        "#scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n",
        "# Mixed Precision, if you need it\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmc6_4eWL2Xp"
      },
      "source": [
        "## Decode Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHjnCDddL36E"
      },
      "outputs": [],
      "source": [
        "def decode_prediction(output, output_lens, decoder, PHONEME_MAP= LABELS):\n",
        "    \n",
        "    # TODO: look at docs for CTC.decoder and find out what is returned here. Check the shape of output and expected shape in decode.\n",
        "    beam_results, beam_scores, timesteps, out_seq_len = decoder.decode(output, seq_lens= output_lens) #lengths - list of lengths\n",
        "\n",
        "    pred_strings                    = []\n",
        "    \n",
        "    for i in range(output_lens.shape[0]):\n",
        "        #TODO: Create the prediction from the output of decoder.decode. Don't forget to map it using PHONEMES_MAP.\n",
        "        beam_slice = beam_results[i,0,:out_seq_len[i,0]]\n",
        "        pred_str = []\n",
        "        pred_str = [PHONEME_MAP[i] for i in beam_slice]\n",
        "        pred_strings.append(pred_str)\n",
        "    \n",
        "    return pred_strings\n",
        "\n",
        "def calculate_levenshtein(output, label, output_lens, label_lens, decoder, PHONEME_MAP= LABELS): # y - sequence of integers\n",
        "    \n",
        "    dist            = 0\n",
        "    batch_size      = label.shape[0]\n",
        "\n",
        "    pred_strings    = decode_prediction(output, output_lens, decoder, PHONEME_MAP)\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        # TODO: Get predicted string and label string for each element in the batch\n",
        "        pred_string = pred_strings[i]#TODO\n",
        "        label_slice = label[i,0:label_lens[i]] #TODO \n",
        "        label_string = [PHONEME_MAP[k] for k in label_slice]\n",
        "        dist += Levenshtein.distance(pred_string, label_string)\n",
        "\n",
        "    dist /= batch_size # TODO: Uncomment this, but think about why we are doing this\n",
        "    # raise NotImplemented\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnTLL-5gMBrY",
        "outputId": "90893d0e-ef5f-40de-8ee3-1543c42cc416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 593, 41])\n",
            "torch.Size([593, 64, 41]) torch.Size([64, 269])\n",
            "tensor(25.3000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(25.3000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "153.484375\n"
          ]
        }
      ],
      "source": [
        "# test code to check shapes\n",
        "\n",
        "model.eval()\n",
        "for i, data in enumerate(val_loader, 0):\n",
        "    x, y, lx, ly = data\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    h, lh = model(x, lx)\n",
        "    print(h.shape)\n",
        "    h = torch.permute(h, (1, 0, 2))\n",
        "    print(h.shape, y.shape)\n",
        "    loss = criterion(h, y, lh, ly)\n",
        "    print(loss)\n",
        "    print(loss)\n",
        "    h = torch.permute(h, (1, 0, 2))\n",
        "\n",
        "    print(calculate_levenshtein(h, y, lx, ly, decoder, LABELS))\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd5aNaLVoR_g"
      },
      "source": [
        "## wandb\n",
        "\n",
        "You will need to fetch your api key from wandb.ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiDduMaDIARE",
        "outputId": "1bb98603-5fcf-469b-e113-070b847035fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login(key=\"cce3530b8ee00a25da9393f98f6b0142435348a3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "4s52yBOvICPZ",
        "outputId": "ce0354d5-00e1-4ed6-cc66-a3c890ee51c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkrana\u001b[0m (\u001b[33mcmu-idl\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230422_014433-po0488fe</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cmu-idl/hw3p2-krana/runs/po0488fe' target=\"_blank\">pblstm-model-nlayers3-conv2-dp-2-ckpt-sub</a></strong> to <a href='https://wandb.ai/cmu-idl/hw3p2-krana' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cmu-idl/hw3p2-krana' target=\"_blank\">https://wandb.ai/cmu-idl/hw3p2-krana</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cmu-idl/hw3p2-krana/runs/po0488fe' target=\"_blank\">https://wandb.ai/cmu-idl/hw3p2-krana/runs/po0488fe</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = wandb.init(\n",
        "    name = \"pblstm-model-nlayers3-conv2-dp-2-ckpt-sub\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    #run_id = \"zputl6zb\", ### Insert specific run id here if you want to resume a previous run\n",
        "    #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw3p2-krana\", ### Project should be created in your wandb account \n",
        "    config = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fLLj5KIMMOe"
      },
      "source": [
        "# Train Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri87MAdhMUz5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer):\n",
        "    \n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast():     \n",
        "            h, lh = model(x, lx)\n",
        "            h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "\n",
        "        if not torch.isfinite(loss).all():\n",
        "            print(f'WARNING: non-finite loss, skipping update for batch {i}')\n",
        "            print(loss)\n",
        "            print(lh >= ly)\n",
        "            continue\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "        # Another couple things you need for FP16. \n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() # This is something added just for FP16\n",
        "\n",
        "        del x, y, lx, ly, h, lh, loss \n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "    \n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def validate_model(model, val_loader, decoder, phoneme_map= LABELS):\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    total_loss = 0\n",
        "    vdist = 0\n",
        "\n",
        "    for i, data in enumerate(val_loader):\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            h, lh = model(x, lx)\n",
        "            h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += float(loss)\n",
        "        vdist += calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh, ly, decoder, phoneme_map)\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(vdist / (i + 1))))\n",
        "\n",
        "        batch_bar.update()\n",
        "    \n",
        "        del x, y, lx, ly, h, lh, loss\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "    batch_bar.close()\n",
        "    total_loss = total_loss/len(val_loader)\n",
        "    val_dist = vdist/len(val_loader)\n",
        "    return total_loss, val_dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGrEFE38MUz5",
        "outputId": "e40651dc-0c68-4679-a898-72d149aaca0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 676, 41])\n",
            "torch.Size([676, 64, 41]) torch.Size([64, 226])\n",
            "tensor(27.3597, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "168.109375\n"
          ]
        }
      ],
      "source": [
        "# test code to check shapes# test code to check shapes\n",
        "\n",
        "model.eval()\n",
        "for i, data in enumerate(val_loader, 0):\n",
        "    x, y, lx, ly = data\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    h, lh = model(x, lx)\n",
        "    print(h.shape)\n",
        "    h = torch.permute(h, (1, 0, 2))\n",
        "    print(h.shape, y.shape)\n",
        "    loss = criterion(h, y, lh, ly)\n",
        "    print(loss)\n",
        "    h = torch.permute(h, (1, 0, 2))\n",
        "\n",
        "    print(calculate_levenshtein(h, y, lx, ly, decoder, LABELS))\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpYExu4vT4_g"
      },
      "source": [
        "### Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "husa5_EYMUz6"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         metric[0]                  : metric[1], \n",
        "         'epoch'                    : epoch}, \n",
        "         path\n",
        "    )\n",
        "\n",
        "def load_model(path, model, metric= 'valid_acc', optimizer= None, scheduler= None):\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer != None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scheduler != None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        \n",
        "    epoch   = checkpoint['epoch']\n",
        "    metric  = checkpoint[metric]\n",
        "\n",
        "    return [model, optimizer, scheduler, epoch, metric]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tExvyl1BIdMC"
      },
      "outputs": [],
      "source": [
        "# This is for checkpointing, if you're doing it over multiple sessions\n",
        "\n",
        "last_epoch_completed = 0\n",
        "start = last_epoch_completed\n",
        "end = config[\"epochs\"]\n",
        "best_lev_dist = float(\"inf\") # if you're restarting from some checkpoint, use what you saw there.\n",
        "epoch_model_path = 'chekpoint.pth'#TODO set the model path( Optional, you can just store best one. Make sure to make the changes below )\n",
        "best_model_path = 'best_model.pth'#TODO set best model path "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JR43E28rM9Ak"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(decoder)\n",
        "#TODO: Please complete the training loop\n",
        "\n",
        "for epoch in range(0, config['epochs']):\n",
        "\n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
        "    \n",
        "    # curr_lr = config['lr'] #TODO\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    train_loss  = train_model(model, train_loader, criterion, optimizer)             #TODO\n",
        "    valid_loss, valid_dist  = validate_model(model, val_loader, decoder) #TODO\n",
        "    scheduler.step(valid_dist)\n",
        "\n",
        "    print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
        "    print(\"\\tVal Dist {:.04f}%\\t Val Loss {:.04f}\".format(valid_dist, valid_loss))\n",
        "\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss,  \n",
        "        'valid_dist': valid_dist, \n",
        "        'valid_loss': valid_loss, \n",
        "        'lr'        : curr_lr\n",
        "    })\n",
        "    \n",
        "    save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, epoch_model_path)\n",
        "    wandb.run.save(epoch_model_path)\n",
        "    print(\"Saved epoch model\")\n",
        "\n",
        "    if valid_dist <= best_lev_dist:\n",
        "        best_lev_dist = valid_dist\n",
        "        save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, best_model_path)\n",
        "        wandb.run.save(best_model_path)\n",
        "        print(\"Saved best model\")\n",
        "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2H4EEj-sD32"
      },
      "source": [
        "# Generate Predictions and Submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2moYJhTWsOG-",
        "outputId": "ddf32dde-d72e-4450-f345-cdf73b530e9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 41/41 [00:26<00:00,  1.55it/s]\n"
          ]
        }
      ],
      "source": [
        "#TODO: Make predictions\n",
        "\n",
        "# Follow the steps below:\n",
        "# 1. Create a new object for CTCBeamDecoder with larger (why?) number of beams\n",
        "# 2. Get prediction string by decoding the results of the beam decoder\n",
        "\n",
        "TEST_BEAM_WIDTH = 10\n",
        "\n",
        "test_decoder    = CTCBeamDecoder(LABELS, beam_width=TEST_BEAM_WIDTH, log_probs_input=True)#TODO \n",
        "test_results = []\n",
        "model.eval()\n",
        "print(\"Testing\")\n",
        "for data in tqdm(test_loader):\n",
        "\n",
        "    x, lx   = data\n",
        "    x       = x.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h, lh = model(x, lx)\n",
        "\n",
        "    prediction_string = decode_prediction(h, lh, test_decoder)\n",
        "    #TODO save the output in results array.\n",
        "    test_results.extend(prediction_string)\n",
        "    \n",
        "    del x, lx, h, lh\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPrB3L1cJISX"
      },
      "outputs": [],
      "source": [
        "test_results_new = [\"\".join(arr) for arr in test_results]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d70dvu_lsMlv",
        "outputId": "ab511736-8805-4c3f-c712-892808e24711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.13 / client 1.5.8)\n",
            "100% 210k/210k [00:02<00:00, 78.7kB/s]\n",
            "Successfully submitted to Automatic Speech Recognition (ASR)"
          ]
        }
      ],
      "source": [
        "data_dir = '/content/11-785-s23-hw3p2/test-clean/random_submission.csv'\n",
        "df = pd.read_csv(data_dir)\n",
        "df.label = test_results_new\n",
        "df.to_csv('submission.csv', index = False)\n",
        "\n",
        "!kaggle competitions submit -c 11-785-s23-hw3p2 -f submission.csv -m \"I made it!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmaFmFYuItV-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10 (main, Feb 16 2023, 02:49:39) [Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}