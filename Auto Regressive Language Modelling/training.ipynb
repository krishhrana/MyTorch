{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HW4P1: Language Modelling\n",
        "\n",
        "Welcome to the final part 1 hw of this course. This is the only part 1 in which you have PyTorch training (Yay). You will be working on training language models and evaluating them on the task of prediction and generation.<br>\n",
        "Note: A major change which we have made this semester is that we have made the model which you will be coding in this HW very similar to the Speller module from HW4P2. "
      ],
      "metadata": {
        "id": "PSLkT0qL3jgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get modules and datasets"
      ],
      "metadata": {
        "id": "EB2bOV3bzYLR",
        "cell_id": "95e48c7693e34a389da49dcb6e448e0c",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummaryX"
      ],
      "metadata": {
        "id": "r4_-qG9rSULt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Import drive if you are using Colab"
      ],
      "metadata": {
        "id": "RnrUvEIC5i5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "INh9p3v3zbF_",
        "cell_id": "65e59cd2e6514d9594258167a6a0f6db",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"<TODO>\") # TODO: Add path to handout/"
      ],
      "metadata": {
        "id": "QZNwme4320LW",
        "cell_id": "03bf3bd639a048f098d5febc42e2baff",
        "source_hash": "b7876178",
        "execution_start": 1679856365820,
        "execution_millis": 4,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "\n",
        "import os\n",
        "\n",
        "import time \n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import torchsummaryX\n",
        "\n",
        "# Importing necessary modules from hw4\n",
        "from hw4.tests_hw4 import test_prediction, test_generation\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device: \", DEVICE)\n",
        "\n",
        "%cd handout # "
      ],
      "metadata": {
        "id": "oxiZ42B4SwQ-",
        "tags": [],
        "cell_id": "b48a9e95f26c4d2e89d95b1b311cedd5",
        "execution": {
          "iopub.status.busy": "2022-08-10T14:02:09.987693Z",
          "iopub.status.idle": "2022-08-10T14:02:12.872562Z",
          "iopub.execute_input": "2022-08-10T14:02:09.992480Z",
          "shell.execute_reply": "2022-08-10T14:02:12.870819Z",
          "shell.execute_reply.started": "2022-08-10T14:02:09.991351Z"
        },
        "source_hash": "ec149d26",
        "execution_start": 1679856365830,
        "execution_millis": 2669,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9888f04a-bc26-4830-d89d-5f0ab94fa875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n",
            "/content/handout\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load datasets"
      ],
      "metadata": {
        "id": "u-R794-0zc9V",
        "cell_id": "a4ff875589ee46da8f749a7e5088a3ef",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the vocabulary. Try printing and see\n",
        "VOCAB       = np.load('dataset/vocab.npy') \n",
        "\n",
        "# We have also included <sos> and <eos> in the vocabulary for you\n",
        "# However in real life, you include it explicitly if not provided\n",
        "SOS_TOKEN   = np.where(VOCAB == '<sos>')[0][0]\n",
        "EOS_TOKEN   = np.where(VOCAB == '<eos>')[0][0]\n",
        "NUM_WORDS   = len(VOCAB) - 2 # Actual number of words in vocabulary\n",
        "\n",
        "print(\"Vocab length: \", len(VOCAB))\n",
        "print(VOCAB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HU4e_6l0Whda",
        "outputId": "bd0d665c-4ee9-4856-aeba-1d74df960d89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab length:  33280\n",
            "['!' '\"' '#' ... 'ï½ž' '<sos>' '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loding the training dataset. Refer to write up section 2 to understand the structure\n",
        "dataset     = np.load('dataset/wiki.train.npy', allow_pickle=True)\n",
        "\n",
        "# The dataset does not have <sos> and <eos> because they are just regular articles. \n",
        "# TODO: Add <sos> and <eos> to every article in the dataset.\n",
        "# Before doing do, try printing the dataset to see if they are words or integers."
      ],
      "metadata": {
        "id": "LA7SapmyXHr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the fixtures for validation and test - prediction\n",
        "fixtures_pred       = np.load('fixtures/prediction.npz')        # validation\n",
        "fixtures_pred_test  = np.load('fixtures/prediction_test.npz')   # test\n",
        "\n",
        "print(\"Validation shapes    : \", fixtures_pred['inp'].shape, fixtures_pred['out'].shape)\n",
        "print(\"Test shapes          : \", fixtures_pred_test['inp'].shape)"
      ],
      "metadata": {
        "id": "x5znxQhLSwRC",
        "tags": [],
        "cell_id": "09f3a2efaeef49ef9f4c0b2b9a614cca",
        "execution": {
          "iopub.status.busy": "2022-08-10T14:02:12.884281Z",
          "iopub.status.idle": "2022-08-10T14:02:12.960590Z",
          "iopub.execute_input": "2022-08-10T14:02:12.888156Z",
          "shell.execute_reply": "2022-08-10T14:02:12.958805Z",
          "shell.execute_reply.started": "2022-08-10T14:02:12.888058Z"
        },
        "source_hash": "42e4c03c",
        "execution_start": 1679856368507,
        "execution_millis": 46,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "994601b9-86c5-4406-b37f-be48b6d3244a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation shapes:  (128, 21) (128,)\n",
            "Test shapes      :  (128, 21)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the fixtures for validation and test - generation\n",
        "fixtures_gen        = np.load('fixtures/generation.npy')        # validation\n",
        "fixtures_gen_test   = np.load('fixtures/generation_test.npy')   # test\n",
        "\n",
        "print(\"Validation Gen Shapes    :\", fixtures_gen.shape)\n",
        "print(\"Test Gen Shapes          :\", fixtures_gen_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pes7mCr5WdAw",
        "outputId": "57199f71-170e-439f-802f-0da5f474c617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Gen Shapes: (32, 21)\n",
            "Test Gen Shapes: (128, 31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Prediction Dev Input and Output\n",
        "# Optional TODO: You can try printing a few samples from the validation set which has both inputs and outputs"
      ],
      "metadata": {
        "id": "jO_Qt7O6rL8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom DataLoader"
      ],
      "metadata": {
        "id": "dHjYhXAOzkrP",
        "cell_id": "aec0165a3f1245dfa52a0cb80dba2578",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoaderForLanguageModeling(torch.utils.data.DataLoader): # Inherit from torch.utils.data.DataLoader\n",
        "    \"\"\"\n",
        "        TODO: Define data loader logic here\n",
        "    \"\"\"\n",
        "    # TODO: You can probably add more parameters as well. Eg. sequence length\n",
        "    def __init__(self, dataset, batch_size, shuffle= True, drop_last= False): \n",
        "        \n",
        "        # If you remember, these are the standard things which you give while defining a dataloader.\n",
        "        # Now you are just customizing your dataloader\n",
        "        self.dataset    = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle    = shuffle\n",
        "        self.drop_last  = drop_last\n",
        "\n",
        "    def __len__(self):\n",
        "        # What output do you get when you print len(loader)? You get the number of batches\n",
        "        # Your dataset has (579, ) articles and each article has a specified amount of words.\n",
        "        # You concatenate the dataset and then batch parts of it according to the sequence length\n",
        "        # TODO: return the number of batches\n",
        "        # If you are using variable sequence_length, the length might not be fixed \n",
        "        return NotImplemented\n",
        "\n",
        "    def __iter__(self):\n",
        "        # TODOs: \n",
        "        # 1. Shuffle data if shuffle is True\n",
        "        # 2. Concatenate articles and drop extra words\n",
        "        # 3. Divide the concetenated dataset into inputs and targets. How do they vary? \n",
        "        # 4. Reshape the inputs and targets into batches (think about the final shape)\n",
        "        # 5. Loop though the batches and yield the input and target according to the sequence length\n",
        "\n",
        "        if self.shuffle:\n",
        "            # TODO\n",
        "            NotImplemented\n",
        "\n",
        "        num_batches = NotImplemented\n",
        "        \n",
        "        batch_idx = 0\n",
        "        if self.drop_last:\n",
        "            # TODO\n",
        "            NotImplemented\n",
        "\n",
        "        while batch_idx < num_batches:\n",
        "            yield NotImplemented"
      ],
      "metadata": {
        "id": "OZNrJ8XvSwRF",
        "tags": [],
        "cell_id": "b2e63a7f6dec4a3f98588725a72a8ff2",
        "execution": {
          "iopub.status.busy": "2022-08-10T14:02:13.078847Z",
          "iopub.status.idle": "2022-08-10T14:02:13.196189Z",
          "iopub.execute_input": "2022-08-10T14:02:13.079390Z",
          "shell.execute_reply": "2022-08-10T14:02:13.192167Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.079324Z"
        },
        "source_hash": "a81eaa14",
        "execution_start": 1679856368575,
        "execution_millis": 48,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Some sanity checks\n",
        "\n",
        "dl = DataLoaderForLanguageModeling(\n",
        "    dataset     = dataset, \n",
        "    batch_size  = 32, \n",
        "    shuffle     = True, \n",
        "    drop_last   = True\n",
        "    # Input Extra parameters here if needed\n",
        ")\n",
        "\n",
        "inputs, targets = next(iter(dl))\n",
        "print(inputs.shape, targets.shape)\n",
        "\n",
        "for x, y in dl:\n",
        "    print(\"x: \", [VOCAB[i] for i in x[0, :]])\n",
        "    print(\"y: \", [VOCAB[i] for i in y[0, :]])\n",
        "    break"
      ],
      "metadata": {
        "id": "fBZSzmy10M9M",
        "cell_id": "773573c8374048d4bcb5a67b905ee2e0",
        "source_hash": "27952b8c",
        "execution_start": 1679856368714,
        "execution_millis": 3,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LanguageModel"
      ],
      "metadata": {
        "id": "WcWU0YlnzmVM",
        "cell_id": "0e75c3c3318d481aa99230d81eb68c13",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here comes the main portion of this HW.\n",
        "# You can do this with a regular LSTM similar to HW3P2. \n",
        "# However, using LSTMCells will make this Language model very similar to the decoder in HW4P2 and we recommend you use that for writing resuable code.\n",
        "\n",
        "class LanguageModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, ): # TODO: Add more parameters if you want\n",
        "        super().__init__()\n",
        "\n",
        "        # For all the layers which you will define, please read the documentation thoroughly before implementation\n",
        "\n",
        "        self.token_embedding    = NotImplemented # TODO: Define a PyTorch embedding layer \n",
        "\n",
        "        self.lstm_cells         = torch.nn.Sequential(\n",
        "            torch.nn.LSTMCell() # TODO: Enter the parameters for the LSTMCells\n",
        "            # You can add multiple LSTMCells too if you want\n",
        "        )\n",
        "\n",
        "        self.token_probability  = torch.nn.Linear() # TODO: Define the parameters\n",
        "\n",
        "        # Optional TODO: Weight Tying. You just need to make the embedding layer weights equal to the Linear layer weight. \n",
        "                \n",
        "        # So the basic pipline is:\n",
        "        # word -> embedding -> lstm -> projection (linear) to get  probability distribution\n",
        "        # And this is happening across all time steps\n",
        "\n",
        "    def rnn_step(self, embedding, hidden_states_list):\n",
        "\n",
        "        for i in range(len(self.lstm_cells)):\n",
        "            # TODO: Forward pass through each LSTMCell\n",
        "            NotImplemented\n",
        "            \n",
        "        return embedding, hidden_states_list\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Refer to Section 1.3.1 to understand this function\n",
        "        if not torch.is_tensor(x):\n",
        "            x = torch.tensor(x).long().to(DEVICE)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            # TODO: Pass the input sequence through the model \n",
        "            # and return the probability distribution of the last timestep\n",
        "            return NotImplemented\n",
        "\n",
        "    def generate(self, x, timesteps): \n",
        "        # Refer to section 1.3.2 to understand this function\n",
        "        # Important Note: We do not draw <eos> from the distribution unlike the writeup\n",
        "        if not torch.is_tensor(x):\n",
        "            x = torch.tensor(x).long().to(DEVICE)\n",
        "\n",
        "        # TODO: Pass the input sequence through the model \n",
        "        # Obtain the probability distribution and hidden_states_list of the last timestep\n",
        "        \n",
        "        token_prob_dist, hidden_states_list     = self.forward(x)\n",
        "        next_token                              = NotImplemented # TODO: Draw the next predicted token from the probability distribution ()  \n",
        "\n",
        "        generated_sequence  = [] \n",
        "        with torch.inference_mode():\n",
        "            for t in range(timesteps): # Loop through the timesteps\n",
        "                #   TODO: Pass the next_token and hidden_states_list through the model\n",
        "                #   TODO: You will get 2 outputs. What is the shape of the probability distribution?\n",
        "                #   TODO: Get the most probable token for the next timestep\n",
        "\n",
        "                generated_sequence.append(next_token)\n",
        "            \n",
        "            generated_sequence = torch.stack(generated_sequence, dim= 1) # keep last timesteps generated words\n",
        "\n",
        "        return generated_sequence\n",
        "\n",
        "    # We are also having a hidden_states_list parameter because you need that in generation\n",
        "    def forward(self, x, hidden_states_list= None): # train model\n",
        "        # x (Batch, Seq_len)\n",
        "        # Note: you dont have to return the sum of log probabilities according to Pseudocode 1 in the writeup\n",
        "        # However, feel free to calculate and print it if you are curious\n",
        "\n",
        "        batch_size, timesteps   = x.shape \n",
        "\n",
        "        token_prob_distribution = [] # list which will contain probability distributions for all timesteps\n",
        "        # Initializing the hidden states\n",
        "        hidden_states_list      = [None]*len(self.lstm_cells) if hidden_states_list == None else hidden_states_list       \n",
        "\n",
        "        token_embeddings        = NotImplemented # TODO\n",
        "        # When you get the embeddings of the input x, remember that you get it for all time steps.\n",
        "        # Embedding is just a linear transformation so you can precompute it for all time steps.\n",
        "\n",
        "        for t in range(timesteps): # LSTMCell is for just 1 timestep. Hence you need to loop through the total timesteps\n",
        "\n",
        "            token_embedding_t           = NotImplemented # TODO  \n",
        "\n",
        "            rnn_out, hidden_states_list = NotImplemented # TODO (What should you do with the hidden_states_list?)\n",
        "            \n",
        "            token_prob_dist_t           = NotImplemented # TODO\n",
        "\n",
        "            token_prob_distribution.append(token_prob_dist_t) \n",
        "\n",
        "        token_prob_distribution = NotImplemented # TODO: Stack along the timesteps dimension\n",
        "\n",
        "        return token_prob_distribution, hidden_states_list "
      ],
      "metadata": {
        "id": "cebwoorWttWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer Class"
      ],
      "metadata": {
        "id": "TlWF_bpLznup",
        "cell_id": "8ed5a6ef54f9446fab752b79c70a0216",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unlike all the P2s, we are using a Trainer class for this HW.\n",
        "# Many researchers also use classes like this for training. You may have encountered them in your project as well.\n",
        "# You dont have to complete everything in this class, you only need to complete the train function.\n",
        "# However, its good to go through the code and see what it does. \n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, loader, optimizer, criterion, scheduler, max_epochs= 1, run_id= 'exp'):\n",
        "        \"\"\"\n",
        "            Use this class to train your model\n",
        "        \"\"\"\n",
        "        # feel free to add any other parameters here\n",
        "        self.model      = model\n",
        "        self.loader     = loader\n",
        "        self.optimizer  = optimizer\n",
        "        self.criterion  = criterion\n",
        "\n",
        "        self.train_losses           = []\n",
        "        self.val_losses             = []\n",
        "        self.predictions            = []\n",
        "        self.predictions_test       = []\n",
        "        self.generated_logits       = []\n",
        "        self.generated              = []\n",
        "        self.generated_logits_test  = []\n",
        "        self.generated_test         = []\n",
        "        self.epochs                 = 0\n",
        "        self.max_epochs             = max_epochs\n",
        "        self.run_id                 = run_id\n",
        "\n",
        "\n",
        "    def calculate_loss(self, out, target):\n",
        "        # output: (B, T, Vocab_size) - probability distributions\n",
        "        # target: (B, T)\n",
        "        # Read the documentation of CrossEntropyLoss and try to understand how it takes inputs\n",
        "\n",
        "        # Tip: If your target is of shape (B, T) it means that you have B batches with T words. \n",
        "        # Tip: What is the total number of words in this batch? \n",
        "        # Tip: Crossentropy calculates the loss between a label and its probability distribution.\n",
        "\n",
        "        out     = NotImplemented # TODO\n",
        "        targets = NotImplemented # TODO\n",
        "        loss    = self.criterion(out, targets)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        self.model.train() # set to training mode\n",
        "        self.model.to(DEVICE)\n",
        "        epoch_loss  = 0\n",
        "        num_batches = 0\n",
        "        \n",
        "        for batch_num, (inputs, targets) in enumerate(tqdm(self.loader)):\n",
        "\n",
        "            # TODO: Complete the loop. You should be able to complete this without any helper comments after 3 HWs\n",
        "            # Tip: Mixed precision training\n",
        "            # For loss calculation, use the calculate_loss function. You need to complete it before using.\n",
        " \n",
        "            loss = loss.item()\n",
        "            epoch_loss += loss\n",
        "        \n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        self.epochs += 1\n",
        "        print('[TRAIN] \\tEpoch [%d/%d] \\tLoss: %.4f \\tLr: %.6f'\n",
        "                      % (self.epochs, self.max_epochs, epoch_loss, self.optimizer.param_groups[0]['lr']))\n",
        "        self.train_losses.append(epoch_loss)\n",
        "\n",
        "\n",
        "    \n",
        "    def test(self): # Don't change this function\n",
        "        \n",
        "        self.model.eval() # set to eval mode\n",
        "        predictions     = model.predict(fixtures_pred['inp']).detach().cpu().numpy() # get predictions\n",
        "        self.predictions.append(predictions)\n",
        "\n",
        "        generated_logits        = model.generate(fixtures_gen, 10).detach().cpu().numpy() # generated predictions for 10 words\n",
        "        generated_logits_test   = model.generate(fixtures_gen_test, 10).detach().cpu().numpy()\n",
        "\n",
        "        nll             = test_prediction(predictions, fixtures_pred['out'])\n",
        "        generated       = test_generation(fixtures_gen, generated_logits, VOCAB)\n",
        "        generated_test  = test_generation(fixtures_gen_test, generated_logits_test, VOCAB)\n",
        "        self.val_losses.append(nll)\n",
        "        \n",
        "        self.generated.append(generated)\n",
        "        self.generated_test.append(generated_test)\n",
        "        self.generated_logits.append(generated_logits)\n",
        "        self.generated_logits_test.append(generated_logits_test)\n",
        "        \n",
        "        # generate predictions for test data\n",
        "        predictions_test = model.predict(fixtures_pred_test['inp']).detach().cpu().numpy() # get predictions\n",
        "        self.predictions_test.append(predictions_test)\n",
        "            \n",
        "        print('[VAL] \\tEpoch [%d/%d] \\tLoss: %.4f'\n",
        "                      % (self.epochs, self.max_epochs, nll))\n",
        "        return nll\n",
        "\n",
        "    \n",
        "    def save(self): # Don't change this function\n",
        "\n",
        "        model_path = os.path.join('hw4/experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "        torch.save({'state_dict': self.model.state_dict()}, model_path)\n",
        "        np.save(os.path.join('hw4/experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
        "        np.save(os.path.join('hw4/experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
        "        np.save(os.path.join('hw4/experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
        "        np.save(os.path.join('hw4/experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
        "        \n",
        "        with open(os.path.join('hw4/experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated[-1])\n",
        "\n",
        "        with open(os.path.join('hw4/experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated_test[-1])"
      ],
      "metadata": {
        "id": "kIvZOIfjSwRK",
        "tags": [],
        "cell_id": "8ea986fc372643389d1ab4c445659e9d",
        "execution": {
          "iopub.status.busy": "2022-08-10T14:02:13.440281Z",
          "iopub.status.idle": "2022-08-10T14:02:13.644455Z",
          "iopub.execute_input": "2022-08-10T14:02:13.440820Z",
          "shell.execute_reply": "2022-08-10T14:02:13.642614Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.440752Z"
        },
        "source_hash": "451a140f",
        "deepnote_to_be_reexecuted": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment setup"
      ],
      "metadata": {
        "id": "E6NKG0j8zsv-",
        "cell_id": "db5de3ac0c6e48ca9cff0cd79bef7ae8",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: define other hyperparameters here\n",
        "\n",
        "configs = dict(\n",
        "    batch_size  = 256,\n",
        "    num_epochs  = 10, # 10 or 20 epochs should be enough given the model is good\n",
        "\n",
        "    init_lr     = NotImplemented # TODO\n",
        ")"
      ],
      "metadata": {
        "id": "TiUrjbEjSwRQ",
        "tags": [],
        "cell_id": "7fc44ee4771a42f996d0a00d35529fb6",
        "execution": {
          "iopub.status.busy": "2022-08-10T14:02:13.850633Z",
          "iopub.status.idle": "2022-08-10T14:02:13.927227Z",
          "iopub.execute_input": "2022-08-10T14:02:13.852171Z",
          "shell.execute_reply": "2022-08-10T14:02:13.924500Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.852093Z"
        },
        "source_hash": "f7524436",
        "deepnote_to_be_reexecuted": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model       = NotImplemented # TODO: Define the model\n",
        "\n",
        "loader      = DataLoaderForLanguageModeling() # TODO: Define the dataloader\n",
        "\n",
        "criterion   = torch.nn.CrossEntropyLoss() \n",
        "\n",
        "optimizer   = NotImplemented # TODO: Define the optimizer. Adam/AdamW usually works good for this HW\n",
        "\n",
        "# Optional TODO: Use a scheduler if you want\n",
        "\n",
        "print(model)\n",
        "torchsummaryX.summary(model, x = inputs.to(DEVICE))"
      ],
      "metadata": {
        "id": "DbHH6zXTSwRa",
        "tags": [],
        "cell_id": "4aaccf1c32fa480a9a15e8bb8bc4d9e4",
        "execution": {
          "iopub.status.busy": "2022-08-10T14:02:14.109778Z",
          "iopub.status.idle": "2022-08-10T14:02:14.929087Z",
          "iopub.execute_input": "2022-08-10T14:02:14.110787Z",
          "shell.execute_reply": "2022-08-10T14:02:14.925078Z",
          "shell.execute_reply.started": "2022-08-10T14:02:14.110707Z"
        },
        "source_hash": "2acff566",
        "deepnote_to_be_reexecuted": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Dont change this cell\n",
        "\n",
        "run_id = str(int(time.time()))\n",
        "if not os.path.exists('./hw4/experiments'):\n",
        "    os.mkdir('./hw4/experiments')\n",
        "os.mkdir('./hw4/experiments/%s' % run_id)\n",
        "print(\"Saving models, predictions, and generated words to ./hw4/experiments/%s\" % run_id)\n",
        "\n",
        "# The object of the Trainer class takes in everything\n",
        "trainer = Trainer(\n",
        "    model       = model, \n",
        "    loader      = loader, \n",
        "\n",
        "    optimizer   = optimizer,\n",
        "    criterion   = criterion, \n",
        "    \n",
        "    max_epochs  = configs['num_epochs'], \n",
        "    run_id      = run_id\n",
        ")"
      ],
      "metadata": {
        "id": "2HCVG5YISwRW",
        "tags": [],
        "cell_id": "aaff53cf948e44b7b9bd49cbcad0ac58",
        "execution": {
          "iopub.status.busy": "2022-08-10T14:02:13.930204Z",
          "iopub.status.idle": "2022-08-10T14:02:14.107883Z",
          "iopub.execute_input": "2022-08-10T14:02:13.931258Z",
          "shell.execute_reply": "2022-08-10T14:02:14.105987Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.931185Z"
        },
        "source_hash": "c9f4594a",
        "deepnote_to_be_reexecuted": true,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the experiments loop. \n",
        "# Each epoch wont take more than 2-3min. If its taking more time, it might be due to (but not limited to) the following:\n",
        "#   * You might be overlapping batches \n",
        "#       Eg. Input: \"I had biryani for lunch today\" and sequence length = 3,\n",
        "#           --> \"I had biryani\", \"for lunch today\" are ideal examples for inputs\n",
        "#           --> \"I had biryani\", \"had biryani for\", \"biryani for lunch\", ... is just redundant info :')\n",
        "#   * Your length calculation in the dataloader might be wrong\n",
        "# If you haven't had biryani, try it :D \n",
        "\n",
        "%%time\n",
        "best_nll = 1e30 \n",
        "for epoch in range(configs['num_epochs']):\n",
        "    trainer.train()\n",
        "    nll = trainer.test()\n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch+1)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save()    "
      ],
      "metadata": {
        "id": "V0dy4CpXJgN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses[0:10], label='Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i_gYqXq9Jgo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create handin"
      ],
      "metadata": {
        "id": "BBqjqy-EyU27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Generate the handin to submit to autolab\n",
        "!make runid=<ENTER RUN ID> epoch=<ENTER EPOCH NUMBER>"
      ],
      "metadata": {
        "id": "hWRyPvWmgLQs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "EB2bOV3bzYLR",
        "INh9p3v3zbF_",
        "u-R794-0zc9V"
      ]
    },
    "deepnote": {},
    "gpuClass": "standard",
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.12",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "nbconvert_exporter": "python"
    },
    "deepnote_notebook_id": "989a3c3836794109ac641230122845a3",
    "deepnote_execution_queue": []
  }
}