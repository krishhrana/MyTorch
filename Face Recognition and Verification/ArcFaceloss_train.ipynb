{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCd2mFgEZi3Q",
        "outputId": "726f4660-ac8b-4d0c-adf6-1680ff7cb031"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.9/dist-packages (1.5.1)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.18.0-py2.py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (67.6.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.27.1)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=5d367e084a7c45b5aa6c464ec1599061cd053ed3af13b3ac2ba76179a58d0211\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.18.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_2HLfNZVpI05"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y519Dc7b3HMi",
        "outputId": "d192bf64-cc2c-4682-a979-4058a96b1ea4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchsummary import summary\n",
        "import torchvision #This library is used for image-based operations (Augmentations)\n",
        "import wandb\n",
        "import os\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import glob\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"<username>\",\"key\":\"<key>\"}') \n",
        "    # Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toQaBffVVJ_8",
        "outputId": "f5daad5d-83ac-421b-9e19-fe56f7b0829e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kaggle==1.5.8\n",
            "  Downloading kaggle-1.5.8.tar.gz (59 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/59.2 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.8-py3-none-any.whl size=73272 sha256=2e21d6e8d341c3468f211ac7512432801059e87f76424804ee04e9a19e47d9a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/02/ef/3f8c8d86b8d5388a1d3155876837f1a1a3143ab3fc2ff1ffad\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.13\n",
            "    Uninstalling kaggle-1.5.13:\n",
            "      Successfully uninstalled kaggle-1.5.13\n",
            "Successfully installed kaggle-1.5.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir '/content/data'\n",
        "\n",
        "#!kaggle competitions download -c 11-785-s23-hw2p2-classification\n",
        "#!unzip -qo '11-785-s23-hw2p2-classification.zip' -d '/content/data'\n",
        "\n",
        "!kaggle competitions download -c 11-785-s23-hw2p2-classification\n",
        "!kaggle competitions download -c 11-785-s23-hw2p2-verification\n",
        "!unzip -qo '11-785-s23-hw2p2-classification.zip' -d '/content/data'\n",
        "!unzip -qo '11-785-s23-hw2p2-verification.zip' -d '/content/data'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmGd62xwVPv2",
        "outputId": "cf82019e-1a5b-4960-d05f-9ddad415561a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 11-785-s23-hw2p2-classification.zip to /content\n",
            "100% 1.71G/1.72G [00:08<00:00, 319MB/s]\n",
            "100% 1.72G/1.72G [00:08<00:00, 221MB/s]\n",
            "Downloading 11-785-s23-hw2p2-verification.zip to /content\n",
            " 30% 5.00M/16.8M [00:00<00:00, 37.7MB/s]\n",
            "100% 16.8M/16.8M [00:00<00:00, 102MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98onvaviViaJ",
        "outputId": "b2878fe7-2a5b-4d00-8947-0190bf87cf49"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'batch_size': 128, # Increase this if your GPU can handle it\n",
        "    'lr': 0.01,\n",
        "    'epochs': 80, # 10 epochs is recommended ONLY for the early submission - you will have to train for much longer typically.\n",
        "    # Include other parameters as needed.\n",
        "}"
      ],
      "metadata": {
        "id": "i9ftZc69Vqbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR    = '/content/data/11-785-s23-hw2p2-classification'# TODO: Path where you have downloaded the data\n",
        "TRAIN_DIR   = os.path.join(DATA_DIR, \"train\") \n",
        "VAL_DIR     = os.path.join(DATA_DIR, \"dev\")\n",
        "\n",
        "# Transforms using torchvision - Refer https://pytorch.org/vision/stable/transforms.html\n",
        "\n",
        "train_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(), \n",
        "    torchvision.transforms.Normalize(mean = [0.5116, 0.4026, 0.3519], std = [0.3073, 0.2697, 0.2587])\n",
        "])# Implementing the right train transforms/augmentation methods is key to improving performance.\n",
        "\n",
        "# Most torchvision transforms are done on PIL images. So you convert it into a tensor at the end with ToTensor()\n",
        "# But there are some transforms which are performed after ToTensor() : e.g - Normalization\n",
        "# Normalization Tip - Do not blindly use normalization that is not suitable for this dataset\n",
        "\n",
        "valid_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(), \n",
        "    torchvision.transforms.Normalize(mean = [0.5116, 0.4026, 0.3519], std = [0.3073, 0.2697, 0.2587])\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset   = torchvision.datasets.ImageFolder(TRAIN_DIR, transform= train_transforms)\n",
        "valid_dataset   = torchvision.datasets.ImageFolder(VAL_DIR, transform= valid_transforms)\n",
        "# You should NOT have data augmentation on the validation set. Why?\n",
        "\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_dataset, \n",
        "    batch_size  = config['batch_size'], \n",
        "    shuffle     = True,\n",
        "    num_workers = 4, \n",
        "    pin_memory  = True\n",
        ")\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = valid_dataset, \n",
        "    batch_size  = config['batch_size'],\n",
        "    shuffle     = False,\n",
        "    num_workers = 2\n",
        ")"
      ],
      "metadata": {
        "id": "vV_4rmcdVllT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of classes    : \", len(train_dataset.classes))\n",
        "print(\"No. of train images  : \", train_dataset.__len__())\n",
        "print(\"Shape of image       : \", train_dataset[0][0].shape)\n",
        "print(\"Batch size           : \", config['batch_size'])\n",
        "print(\"Train batches        : \", train_loader.__len__())\n",
        "print(\"Val batches          : \", valid_loader.__len__())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4K9YEgyIVtV4",
        "outputId": "5d952ea5-6278-4184-9da1-3b81943a8467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes    :  7000\n",
            "No. of train images  :  140000\n",
            "Shape of image       :  torch.Size([3, 224, 224])\n",
            "Batch size           :  128\n",
            "Train batches        :  1094\n",
            "Val batches          :  274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ArcFace Loss\n",
        "\n",
        "[ArcFace: Additive Angular Margin Loss for Deep\n",
        "Face Recognition](https://arxiv.org/pdf/1801.07698.pdf)\n",
        "\n",
        "ArcFace Loss is trying to maximize the geodesic distance on the hypersphere between features of different classes to make the features more separately. Here is a blog that explains ArcFace Loss in detail: [link](https://medium.com/analytics-vidhya/face-recognition-and-arcface-additive-angular-margin-loss-for-deep-face-recognition-44abc56916c#:~:text=The%20ArcFace%20loss%20maximizes%20the,implemented%20with%20negligible%20computational%20overhead)\n",
        "\n",
        "$$L_{afl} = - log \\frac{e^{scos(\\theta_{y_i} + m)}}{e^{s cos(\\theta_{y_i} + m)} + \\sum_{j=1,j \\neq y_i}^N e^{s cos(\\theta_j)}}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "xR8bGXJ0sM20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "class ArcFaceModel(torch.nn.Module):\n",
        "    '''\n",
        "    To train in a standard training loop make sure to modify the train function so you pass in the inputs and the labels\n",
        "    i.e. output = model(images, labels)\n",
        "    '''\n",
        "    def __init__(self, model, margin=0.5, scaler=30, embedding_size=512, num_classes=7000):\n",
        "        super(ArcFaceModel, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # small number to avoid invalid arcCos values\n",
        "        self.eps = 1e-6\n",
        "        \n",
        "        # hyperparameters \n",
        "        self.margin = margin\n",
        "        self.scaler = scaler\n",
        "\n",
        "        # load classification model\n",
        "        self.model = model\n",
        "\n",
        "        # Initializing the arcface linear layer with the weights of the classifier from the trained CNN\n",
        "        self.AFL_linear = torch.nn.Linear(embedding_size, num_classes, bias=False) # Why set bias=False? Check out the paper.\n",
        "        with torch.no_grad():\n",
        "          self.AFL_linear.weight.copy_(self.model.fc.weight)\n",
        "\n",
        "        # Initializing utility functions for normalization, arcCos, cos and onehot encoding\n",
        "        self.normalizer = torch.nn.functional.normalize\n",
        "        self.arcCos = torch.acos\n",
        "        self.cos = torch.cos\n",
        "        self.one_hot = torch.nn.functional.one_hot\n",
        "\n",
        "      \n",
        "    def forward(self, x, labels):\n",
        "        # Get face embedding\n",
        "        embedding = self.model(x, return_feats=True)\n",
        "\n",
        "        # TODO: normalize face embedding\n",
        "        embedding = self.normalizer(embedding, p=2)\n",
        "\n",
        "        # TODO: normalize linear layer weights\n",
        "        with torch.no_grad():\n",
        "          self.AFL_linear.weight = torch.nn.Parameter(self.normalizer(self.AFL_linear.weight, p=2), requires_grad=True)\n",
        "        \n",
        "        # TODO: take dot product to get cos theta, remember that Wx = ||W||||x||cos(\\theta) and ||W|| = 1, ||x|| = 1\n",
        "        \n",
        "        cosine = torch.mm(embedding, self.AFL_linear.weight.T)\n",
        "        \n",
        "        # We clamp the values to be a little higher than -1 and a little lower than one so we don't get nan values when we call arccos\n",
        "        cosine = torch.clamp(cosine, min=-1.0+self.eps, max=1.0-self.eps)\n",
        "   \n",
        "        # TODO: get theta by performing arccos(cos(theta))\n",
        "        theta = self.arcCos(cosine)\n",
        "\n",
        "        # TODO: convert labels to one-hot\n",
        "        one_hot_labels = self.one_hot(labels, num_classes = self.num_classes)\n",
        "        # TODO: create a mask with m at positions with label 1 and 0 at positions with label 0\n",
        "        margin_mask = torch.where(one_hot_labels == 1, self.margin, 0)\n",
        "        # TODO: add margin m to theta\n",
        "        theta_m = theta + margin_mask\n",
        "\n",
        "        # calculate the cosine value for theta with margin added and scale with self.scaler\n",
        "        logits = self.scaler * self.cos(theta_m) # this value is then passed to crossEntropyLoss in train loop to calculate arcface loss\n",
        "        return logits"
      ],
      "metadata": {
        "id": "-IHwavAyLAX9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I have referenced https://blog.paperspace.com/writing-resnet-from-scratch-in-pytorch/ while building ResNet. \n",
        "# The layer names are similar for debugging purposes. \n",
        "\n",
        "class ResidualBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample=None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3,\n",
        "                            stride = stride, padding = 1),\n",
        "            torch.nn.BatchNorm2d(out_channels),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "        self.conv2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3,\n",
        "                                     stride = 1, padding=1),\n",
        "            torch.nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.conv1(x)\n",
        "        output = self.conv2(output)\n",
        "        if self.downsample:\n",
        "            residual = self.downsample(x)\n",
        "        else:\n",
        "            residual = x\n",
        "        output = output + residual\n",
        "        output = self.relu(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "class ResNet(torch.nn.Module):\n",
        "    def __init__(self, block, layers, num_classes = 7000):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.num_channels = 64\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.conv1 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride = 2, padding=3),\n",
        "            torch.nn.BatchNorm2d(self.num_channels),\n",
        "            torch.nn.ReLU())\n",
        "\n",
        "        self.maxpool = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer0 = self.layer_blocks(block, 64, layers[0], stride=1)\n",
        "        self.layer1 = self.layer_blocks(block, 128, layers[1], stride=2)\n",
        "        self.layer2 = self.layer_blocks(block, 256, layers[2], stride=2)\n",
        "        self.layer3 = self.layer_blocks(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = torch.nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = torch.nn.Linear(512, self.num_classes)\n",
        "\n",
        "\n",
        "    def layer_blocks(self, block, num_filters, num_blocks, stride = 1):\n",
        "        downsample = None\n",
        "        if (stride != 1) or (num_filters != self.num_channels):\n",
        "            downsample = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(in_channels=self.num_channels, out_channels=num_filters, kernel_size=1, stride = stride),\n",
        "                torch.nn.BatchNorm2d(num_filters)\n",
        "            )\n",
        "        else:\n",
        "            downsample = None\n",
        "        layers = [block(self.num_channels, num_filters, stride, downsample)]\n",
        "        self.num_channels = num_filters\n",
        "\n",
        "        for i in range(1, num_blocks):\n",
        "            layers.append(block(self.num_channels, num_filters))\n",
        "\n",
        "        return torch.nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer0(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "551eApviQVen"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet(ResidualBlock, layers = [3, 4, 6, 3])\n",
        "checkpoint = torch.load('/content/drive/MyDrive/IDL/HW2P2/models/ResNet34-restart-2.pth', map_location=torch.device('cpu'))\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "arcFaceModel = ArcFaceModel(model, margin=0.5, scaler=64\n",
        "                            , embedding_size=512, num_classes=7000).to(DEVICE)"
      ],
      "metadata": {
        "id": "Da-tuQXsQ2yo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor = 0.2, patience = 2)\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ],
      "metadata": {
        "id": "8udUBllEWtch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, optimizer, criterion):\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    # Progress Bar \n",
        "    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5) \n",
        "\n",
        "    num_correct = 0\n",
        "    total_loss  = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "        \n",
        "        optimizer.zero_grad() # Zero gradients\n",
        "\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "        \n",
        "        with torch.cuda.amp.autocast(): # This implements mixed precision. Thats it! \n",
        "            outputs = model(images, labels)\n",
        "            loss    = criterion(outputs, labels)\n",
        "\n",
        "        # Update no. of correct predictions & loss as we iterate\n",
        "        num_correct     += int((torch.argmax(outputs, axis=1) == labels).sum())\n",
        "        total_loss      += float(loss.item())\n",
        "\n",
        "        # tqdm lets you add some details so you can monitor training as you train.\n",
        "        batch_bar.set_postfix(\n",
        "            acc         = \"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n",
        "            loss        = \"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct = num_correct,\n",
        "            lr          = \"{:.06f}\".format(float(optimizer.param_groups[0]['lr']))\n",
        "        )\n",
        "        \n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() \n",
        "\n",
        "        # TODO? Depending on your choice of scheduler,\n",
        "        # You may want to call some schdulers inside the train function. What are these?\n",
        "      \n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    acc         = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
        "    total_loss  = float(total_loss / len(dataloader))\n",
        "\n",
        "    return acc, total_loss"
      ],
      "metadata": {
        "id": "KKGH-S8JXN7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, dataloader, criterion):\n",
        "  \n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=5)\n",
        "\n",
        "    num_correct = 0.0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "        \n",
        "        # Move images to device\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "        \n",
        "        # Get model outputs\n",
        "        with torch.inference_mode():\n",
        "            outputs = model(images, labels)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n",
        "            loss=\"{:.06f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct=num_correct)\n",
        "\n",
        "        batch_bar.update()\n",
        "        \n",
        "    batch_bar.close()\n",
        "    acc = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
        "    total_loss = float(total_loss / len(dataloader))\n",
        "    return acc, total_loss"
      ],
      "metadata": {
        "id": "TSX6YNZNXSFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This obtains the list of known identities from the known folder\n",
        "known_regex = \"/content/data/11-785-s23-hw2p2-verification/known/*/*\"\n",
        "known_paths = [i.split('/')[-2] for i in sorted(glob.glob(known_regex))]\n",
        "\n",
        "# Obtain a list of images from unknown folders\n",
        "unknown_dev_regex = \"/content/data/11-785-s23-hw2p2-verification/unknown_dev/*\"\n",
        "unknown_test_regex = \"/content/data/11-785-s23-hw2p2-verification/unknown_test/*\"\n",
        "\n",
        "# We load the images from known and unknown folders\n",
        "unknown_dev_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_dev_regex)))]\n",
        "unknown_test_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_test_regex)))]\n",
        "known_images = [Image.open(p) for p in tqdm(sorted(glob.glob(known_regex)))]\n",
        "\n",
        "# Why do you need only ToTensor() here?\n",
        "transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor()])\n",
        "\n",
        "unknown_dev_images = torch.stack([transforms(x) for x in unknown_dev_images])\n",
        "unknown_test_images = torch.stack([transforms(x) for x in unknown_test_images])\n",
        "known_images  = torch.stack([transforms(y) for y in known_images ])\n",
        "#Print your shapes here to understand what we have done\n",
        "\n",
        "# You can use other similarity metrics like Euclidean Distance if you wish\n",
        "similarity_metric = torch.nn.CosineSimilarity(dim= 1, eps= 1e-6) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLv8bEaZwbxF",
        "outputId": "d3a231f5-de28-4a7f-b474-8d285e115c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 360/360 [00:00<00:00, 6400.23it/s]\n",
            "100%|██████████| 720/720 [00:00<00:00, 9621.74it/s]\n",
            "100%|██████████| 960/960 [00:00<00:00, 4469.38it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_verification(unknown_images, known_images, model, similarity, batch_size= config['batch_size'], mode='val'): \n",
        "\n",
        "    unknown_feats, known_feats = [], []\n",
        "\n",
        "    batch_bar = tqdm(total=len(unknown_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n",
        "    model.eval()\n",
        "\n",
        "    # We load the images as batches for memory optimization and avoiding CUDA OOM errors\n",
        "    for i in range(0, unknown_images.shape[0], batch_size):\n",
        "        unknown_batch = unknown_images[i:i+batch_size] # Slice a given portion upto batch_size\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            unknown_feat = model(unknown_batch.float().to(DEVICE), return_feats=True) #Get features from model         \n",
        "        unknown_feats.append(unknown_feat)\n",
        "        batch_bar.update()\n",
        "    \n",
        "    batch_bar.close()\n",
        "    \n",
        "    batch_bar = tqdm(total=len(known_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n",
        "    \n",
        "    for i in range(0, known_images.shape[0], batch_size):\n",
        "        known_batch = known_images[i:i+batch_size] \n",
        "        with torch.no_grad():\n",
        "              known_feat = model(known_batch.float().to(DEVICE), return_feats=True)\n",
        "          \n",
        "        known_feats.append(known_feat)\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "    # Concatenate all the batches\n",
        "    unknown_feats = torch.cat(unknown_feats, dim=0)\n",
        "    known_feats = torch.cat(known_feats, dim=0)\n",
        "\n",
        "    similarity_values = torch.stack([similarity(unknown_feats, known_feature) for known_feature in known_feats])\n",
        "    # Print the inner list comprehension in a separate cell - what is really happening?\n",
        "\n",
        "    max_similarity_values, predictions = similarity_values.max(0) #Why are we doing an max here, where are the return values?\n",
        "    max_similarity_values, predictions = max_similarity_values.cpu().numpy(), predictions.cpu().numpy()\n",
        "\n",
        "\n",
        "    # Note that in unknown identities, there are identities without correspondence in known identities.\n",
        "    # Therefore, these identities should be not similar to all the known identities, i.e. max similarity will be below a certain \n",
        "    # threshold compared with those identities with correspondence.\n",
        "\n",
        "    # In early submission, you can ignore identities without correspondence, simply taking identity with max similarity value\n",
        "    pred_id_strings = [known_paths[i] for i in predictions] # Map argmax indices to identity strings\n",
        "    \n",
        "    # After early submission, remove the previous line and uncomment the following code \n",
        "\n",
        "    threshold = 0.5\n",
        "    NO_CORRESPONDENCE_LABEL = 'n000000'\n",
        "    pred_id_strings = []\n",
        "    for idx, prediction in enumerate(predictions):\n",
        "        if max_similarity_values[idx] < threshold: # why < ? Thank about what is your similarity metric\n",
        "            pred_id_strings.append(NO_CORRESPONDENCE_LABEL)\n",
        "        else:\n",
        "            pred_id_strings.append(known_paths[prediction])\n",
        "  \n",
        "    if mode == 'val':\n",
        "      true_ids = pd.read_csv('/content/data/11-785-s23-hw2p2-verification/verification_dev.csv')['label'].tolist()\n",
        "      accuracy = accuracy_score(pred_id_strings, true_ids)\n",
        "      print(\"Verification Accuracy = {}\".format(accuracy))\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "Oc2NvfjIwWJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect() # These commands help you when you face CUDA OOM error\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "tdH0UqNrZYe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key=\"\") #API Key is in your wandb account, under settings (wandb.ai/settings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp3RPKxYZaJO",
        "outputId": "bebc25f1-6877-410e-f7e2-bbf08068be55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkrana\u001b[0m (\u001b[33mcmu-idl\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create your wandb run\n",
        "run = wandb.init(\n",
        "    name = \"ResNet34-arcFace-freeze\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    #id = 'vo24kiuj', #Insert specific run id here if you want to resume a previous run\n",
        "    #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw2p2-krana\", ### Project should be created in your wandb account \n",
        "    config = config ### Wandb Config for your run\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "Q2VWy79uZbfj",
        "outputId": "70b2c4f5-58cb-4d67-afaf-d8dc1715feac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230317_225633-pum1vkyq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cmu-idl/hw2p2-krana/runs/pum1vkyq' target=\"_blank\">ResNet34-arcFace-freeze</a></strong> to <a href='https://wandb.ai/cmu-idl/hw2p2-krana' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cmu-idl/hw2p2-krana' target=\"_blank\">https://wandb.ai/cmu-idl/hw2p2-krana</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cmu-idl/hw2p2-krana/runs/pum1vkyq' target=\"_blank\">https://wandb.ai/cmu-idl/hw2p2-krana/runs/pum1vkyq</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_valacc = 0.0\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    train_acc, train_loss = train(arcFaceModel, train_loader, optimizer, criterion)\n",
        "    \n",
        "    print(\"\\nEpoch {}/{}: \\nTrain Acc {:.04f}%\\t Train Loss {:.04f}\\t Learning Rate {:.06f}\".format(\n",
        "        epoch + 1,\n",
        "        config['epochs'],\n",
        "        train_acc,\n",
        "        train_loss,\n",
        "        curr_lr))\n",
        "    \n",
        "    val_acc, val_loss = validate(arcFaceModel, valid_loader, criterion)\n",
        "    eval_acc = eval_verification(unknown_dev_images, known_images, model, similarity_metric, config['batch_size'], mode='val')\n",
        "\n",
        "    \n",
        "    print(\"Val Acc {:.04f}%\\t Val Loss {:.04f} \\t Eval Acc {:.04f}\".format(val_acc, val_loss, eval_acc))\n",
        "\n",
        "    wandb.log({\"train_loss\":train_loss, 'train_Acc': train_acc, 'validation_Acc':val_acc, \n",
        "               'validation_loss': val_loss, \"learning_Rate\": curr_lr})\n",
        "    \n",
        "    # If you are using a scheduler in your train function within your iteration loop, you may want to log\n",
        "    # your learning rate differently \n",
        "\n",
        "    # #Save model in drive location if val_acc is better than best recorded val_acc\n",
        "    if val_acc >= best_valacc:\n",
        "      #path = os.path.join(root, model_directory, 'checkpoint' + '.pth')\n",
        "      print(\"Saving model\")\n",
        "      torch.save({'model_state_dict':model.state_dict(),\n",
        "                  'optimizer_state_dict':optimizer.state_dict(),\n",
        "                  #'scheduler_state_dict':scheduler.state_dict(),\n",
        "                  'val_acc': val_acc, \n",
        "                  'epoch': epoch}, '/content/drive/MyDrive/IDL/HW2P2/models-contrastive/ResNet34-arcface.pth')\n",
        "      torch.save({'model_state_dict':model.state_dict(),\n",
        "                  'optimizer_state_dict':optimizer.state_dict(),\n",
        "                  #'scheduler_state_dict':scheduler.state_dict(),\n",
        "                  'val_acc': val_acc, \n",
        "                  'epoch': epoch}, '/content/ResNet34-arcface.pth')\n",
        "      best_valacc = val_acc\n",
        "      wandb.save('ResNet34-arcFace.pth')\n",
        "    scheduler.step(val_loss)\n",
        "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
        "run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Inbcf9hwZdJY",
        "outputId": "e567363b-e267-4216-9316-3881a6c9f10b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.7650\t Learning Rate 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.075\n",
            "Val Acc 0.0000%\t Val Loss 18.7315 \t Eval Acc 0.0750\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.7265\t Learning Rate 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.1361111111111111\n",
            "Val Acc 0.0000%\t Val Loss 18.7304 \t Eval Acc 0.1361\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.7191\t Learning Rate 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.15\n",
            "Val Acc 0.0000%\t Val Loss 18.7276 \t Eval Acc 0.1500\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.7129\t Learning Rate 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.13333333333333333\n",
            "Val Acc 0.0000%\t Val Loss 18.7253 \t Eval Acc 0.1333\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.7085\t Learning Rate 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.12777777777777777\n",
            "Val Acc 0.0000%\t Val Loss 18.7268 \t Eval Acc 0.1278\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.7102\t Learning Rate 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.12777777777777777\n",
            "Val Acc 0.0000%\t Val Loss 18.7202 \t Eval Acc 0.1278\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.7096\t Learning Rate 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.14444444444444443\n",
            "Val Acc 0.0000%\t Val Loss 18.7185 \t Eval Acc 0.1444\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.7071\t Learning Rate 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.14722222222222223\n",
            "Val Acc 0.0000%\t Val Loss 18.7281 \t Eval Acc 0.1472\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.7108\t Learning Rate 0.010000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.08055555555555556\n",
            "Val Acc 0.0000%\t Val Loss 18.7220 \t Eval Acc 0.0806\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.6980\t Learning Rate 0.002000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.10833333333333334\n",
            "Val Acc 0.0000%\t Val Loss 18.7101 \t Eval Acc 0.1083\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.6880\t Learning Rate 0.002000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.1111111111111111\n",
            "Val Acc 0.0000%\t Val Loss 18.7162 \t Eval Acc 0.1111\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.6813\t Learning Rate 0.002000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.11666666666666667\n",
            "Val Acc 0.0000%\t Val Loss 18.6955 \t Eval Acc 0.1167\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.6643\t Learning Rate 0.002000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.125\n",
            "Val Acc 0.0000%\t Val Loss 18.6967 \t Eval Acc 0.1250\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.6578\t Learning Rate 0.002000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.12777777777777777\n",
            "Val Acc 0.0000%\t Val Loss 18.6970 \t Eval Acc 0.1278\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.6497\t Learning Rate 0.002000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.13333333333333333\n",
            "Val Acc 0.0000%\t Val Loss 18.6907 \t Eval Acc 0.1333\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.6364\t Learning Rate 0.002000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.10833333333333334\n",
            "Val Acc 0.0000%\t Val Loss 18.6938 \t Eval Acc 0.1083\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.6240\t Learning Rate 0.002000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.14722222222222223\n",
            "Val Acc 0.0000%\t Val Loss 18.6837 \t Eval Acc 0.1472\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.6188\t Learning Rate 0.002000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.11388888888888889\n",
            "Val Acc 0.0000%\t Val Loss 18.6762 \t Eval Acc 0.1139\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.6066\t Learning Rate 0.002000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.12777777777777777\n",
            "Val Acc 0.0000%\t Val Loss 18.6862 \t Eval Acc 0.1278\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.6046\t Learning Rate 0.002000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.125\n",
            "Val Acc 0.0000%\t Val Loss 18.6723 \t Eval Acc 0.1250\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 21/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.6016\t Learning Rate 0.002000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.13055555555555556\n",
            "Val Acc 0.0000%\t Val Loss 18.6690 \t Eval Acc 0.1306\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 22/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.5949\t Learning Rate 0.002000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.11666666666666667\n",
            "Val Acc 0.0000%\t Val Loss 18.6884 \t Eval Acc 0.1167\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 23/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.5911\t Learning Rate 0.002000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.10555555555555556\n",
            "Val Acc 0.0000%\t Val Loss 18.6859 \t Eval Acc 0.1056\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 24/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.5942\t Learning Rate 0.002000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.07777777777777778\n",
            "Val Acc 0.0000%\t Val Loss 18.6817 \t Eval Acc 0.0778\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 25/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.5606\t Learning Rate 0.000400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.10833333333333334\n",
            "Val Acc 0.0000%\t Val Loss 18.6453 \t Eval Acc 0.1083\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 26/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.5447\t Learning Rate 0.000400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.10833333333333334\n",
            "Val Acc 0.0000%\t Val Loss 18.6426 \t Eval Acc 0.1083\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 27/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.5348\t Learning Rate 0.000400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.11944444444444445\n",
            "Val Acc 0.0000%\t Val Loss 18.6375 \t Eval Acc 0.1194\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 28/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.5248\t Learning Rate 0.000400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.11944444444444445\n",
            "Val Acc 0.0000%\t Val Loss 18.6413 \t Eval Acc 0.1194\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 29/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.5146\t Learning Rate 0.000400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.1111111111111111\n",
            "Val Acc 0.0000%\t Val Loss 18.6468 \t Eval Acc 0.1111\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 30/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.5039\t Learning Rate 0.000400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.11944444444444445\n",
            "Val Acc 0.0000%\t Val Loss 18.6394 \t Eval Acc 0.1194\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 31/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.4922\t Learning Rate 0.000080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.125\n",
            "Val Acc 0.0000%\t Val Loss 18.6428 \t Eval Acc 0.1250\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 32/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.4884\t Learning Rate 0.000080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.12777777777777777\n",
            "Val Acc 0.0000%\t Val Loss 18.6493 \t Eval Acc 0.1278\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 33/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.4854\t Learning Rate 0.000080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.12222222222222222\n",
            "Val Acc 0.0000%\t Val Loss 18.6483 \t Eval Acc 0.1222\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 34/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.4822\t Learning Rate 0.000016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.125\n",
            "Val Acc 0.0000%\t Val Loss 18.6521 \t Eval Acc 0.1250\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 35/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.4815\t Learning Rate 0.000016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.13055555555555556\n",
            "Val Acc 0.0000%\t Val Loss 18.6560 \t Eval Acc 0.1306\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 36/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.4808\t Learning Rate 0.000016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.12777777777777777\n",
            "Val Acc 0.0000%\t Val Loss 18.6521 \t Eval Acc 0.1278\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 37/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.4802\t Learning Rate 0.000003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.12222222222222222\n",
            "Val Acc 0.0000%\t Val Loss 18.6526 \t Eval Acc 0.1222\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 38/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.4801\t Learning Rate 0.000003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.12777777777777777\n",
            "Val Acc 0.0000%\t Val Loss 18.6584 \t Eval Acc 0.1278\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 39/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.4799\t Learning Rate 0.000003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.13055555555555556\n",
            "Val Acc 0.0000%\t Val Loss 18.6537 \t Eval Acc 0.1306\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 40/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.4797\t Learning Rate 0.000001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.125\n",
            "Val Acc 0.0000%\t Val Loss 18.6530 \t Eval Acc 0.1250\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 41/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.4797\t Learning Rate 0.000001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.12777777777777777\n",
            "Val Acc 0.0000%\t Val Loss 18.6588 \t Eval Acc 0.1278\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 42/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.4797\t Learning Rate 0.000001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.11944444444444445\n",
            "Val Acc 0.0000%\t Val Loss 18.6538 \t Eval Acc 0.1194\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 43/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.4797\t Learning Rate 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.12222222222222222\n",
            "Val Acc 0.0000%\t Val Loss 18.6536 \t Eval Acc 0.1222\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 44/80: \n",
            "Train Acc 0.0000%\t Train Loss 18.4797\t Learning Rate 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification Accuracy = 0.13333333333333333\n",
            "Val Acc 0.0000%\t Val Loss 18.6516 \t Eval Acc 0.1333\n",
            "Saving model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:  72%|███████▏  | 787/1094 [01:08<00:26, 11.55it/s, acc=0.0000%, loss=18.4793, lr=0.000000, num_correct=0]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-07dc9f816bf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcurr_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marcFaceModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     print(\"\\nEpoch {}/{}: \\nTrain Acc {:.04f}%\\t Train Loss {:.04f}\\t Learning Rate {:.06f}\".format(\n",
            "\u001b[0;32m<ipython-input-9-53123730e259>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This is a replacement for loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This is a replacement for optimizer.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No inf checks were recorded for this optimizer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             adamw(params_with_grad,\n\u001b[0m\u001b[1;32m    163\u001b[0m                   \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                   \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    220\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gDvrL6jRUSmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SphereFace Loss\n",
        "[SphereFace: Deep Hypersphere Embedding for Face Recognition](https://arxiv.org/pdf/1704.08063.pdf)\n",
        "\n",
        "[SphereFace Revived:\n",
        "Unifying Hyperspherical Face Recognition](https://arxiv.org/pdf/2109.05565.pdf)\n",
        "\n",
        "$$L_{sfl} = - log \\frac{e^{scos(m\\theta_{y_i})}}{e^{s cos(m\\theta_{y_i})} + \\sum_{j=1,j \\neq y_i}^N e^{s cos(\\theta_j)}}$$\n",
        "\n",
        "Notice that the only difference between arcface loss and sphere loss is from $e^{scos(\\theta_{y_i} + m)}$ to $e^{scos(m\\theta_{y_i})}$. You should be able to implement this based on the comments in ArcFace loss and update `margin_mask` variable accordingly.\n",
        "\n"
      ],
      "metadata": {
        "id": "JgYWY_b3reVr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4sHI80FZRGVP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
